{
	"nodes":[
		{"type":"file","file":"Machine Learning.md","id":"22e1a2f895acf05e","x":-282,"y":333,"width":524,"height":400},
		{"type":"file","file":"_AIML MOC.md","id":"956ae810b046fc5c","x":89,"y":-453,"width":400,"height":400},
		{"type":"text","text":"# *Artificial Intelligence* and *Machine Learning*","id":"fcbd7b06fa2b0ee2","x":-492,"y":-1,"width":419,"height":139},
		{"type":"file","file":"Multiple Linear Regression.md","id":"0cbc1ee4808852ff","x":960,"y":1142,"width":694,"height":486},
		{"type":"text","text":"![[Supervised Machine Learning#^5450a3]]","id":"0176eef9c6394291","x":1521,"y":646,"width":387,"height":138},
		{"type":"text","text":"![[Supervised Machine Learning#^376c22]]","id":"476a43ca8fb26d53","x":2167,"y":506,"width":459,"height":178},
		{"type":"file","file":"Supervised Machine Learning.md","subpath":"#Regression","id":"973ee40c93fc1cd9","x":1521,"y":93,"width":449,"height":374},
		{"type":"text","text":"## checking convergence of gradient descent\n\nLet $\\epsilon$ be a small number (say $10^{-3}$ ).\n\nIf $J(W,b) \\le \\epsilon$ : convergence has been reached ","id":"9e980a31b13e06b3","x":2855,"y":2347,"width":419,"height":279},
		{"type":"file","file":"Multiple Linear Regression.md","subpath":"#gradient descent for multiple linear regression","id":"8ba726aeedb83a04","x":980,"y":1927,"width":623,"height":500},
		{"type":"text","text":"```\n[[0.   0. ]\n [0.5  0.5]\n [1.   1. ]]\n```","id":"9209659129726005","x":-1135,"y":2852,"width":196,"height":144},
		{"type":"text","text":"```python\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\n\n# create a toy dataset\nX_train = np.array([[1., 2.],\n                    [3., 4.],\n                    [5., 6.]])\n\n# create a MinMaxScaler object and fit the training data\nscaler = MinMaxScaler()\nscaler.fit(X_train)\n\n# transform the training data using the learned parameters\nX_train_scaled = scaler.transform(X_train)\n\n# print the scaled data\nprint(X_train_scaled)\n\n```","id":"e5b14976b0d7ed55","x":-1294,"y":2125,"width":617,"height":501},
		{"type":"text","text":"## How to perform feature scaling\nIf the values of both $x_1$ and $x_2$ are in the same/comparable range, the contour would be circular.\n\n1. **Min-Max scaling / Normalization**\n\t- maps between 0 and 1\n\t- sensitive to outliers\n\t- $$x_{new}=\\frac{x_i-min(x_i)}{max(x_i)-min(x_i)}$$\n2. Standardization / Z-score scaling\n\t- resulting data will have mean=0, std. dev.=1\n\t- less sensitive to outliers\n\t- $$x_{new}=\\frac{x_i-x_{mean}}{\\sigma}$$\n3. Robust scaling\n4. Logarithmic scaling\n5. Power scaling\n6. Unit vector scaling","id":"45f22145efd1381c","x":-2110,"y":2327,"width":610,"height":669},
		{"type":"text","text":"# Feature Scaling\n\n## Why Feature Scaling is needed?\nWhen a feature of a model has a range of values that is very large (magnitude wise), the learning algorithms sets the corresponding weight very small, and vice versa.\n\nBecause of this, the 2D-contour the cost function on two features (say one which has a large range of values and another with short range) is *elliptical*.\n\nSuppose, \n- $x_1$ is the feature with large range, thus $w_1$ would be small.\n- $x_2$ is the feature with short range, thus $w_2$ would be large.\n\nTherefore,\n- A small change in $w_1$ will cause a huge change in resulting product $w_1x_1$. Thus contour will be narrow across this axis\n- A small change in $w_2$ will cause a small change in resulting product $w_2x_2$. Thus contour will be broad across this axis.\n\nSince, the cost function curve is skewed, when the model is tested, the gradient descent algo will make the weight values bounce/oscillate the narrow axis","id":"72ebb3e3d6b2058b","x":-1964,"y":1414,"width":670,"height":675},
		{"type":"text","text":"# Data Science","id":"27c97ef539264f37","x":-1805,"y":481,"width":511,"height":468},
		{"type":"text","text":"# Feature Engineering\n\nUsing intuition to design *new features* by transforming or combining original features\n\nSuppose for a model,\n$$f_{W,b}(X)=w_1x_1+w_2x_2+b$$\nwhere $x_1$ is length, $x_2$ is breadth and we need to predict the price of the item using the model\n\nAnother effective way to create the model: incorporate the *area* $x_3=x_1x_2$ as well. Then the model would be\n$$f_{W,b}(X)=w_1x_1+w_2x_2+w_3x_3+b$$\n\nThis makes it easier for the model to make accurate predictions","id":"6c20ba169c6e4188","x":-2843,"y":614,"width":637,"height":513},
		{"type":"file","file":"screenshots/Pasted image 20230309194428.png","id":"b46217417c022682","x":-3843,"y":2375,"width":1362,"height":345},
		{"type":"file","file":"screenshots/Pasted image 20230309194213.png","id":"a9a6a839bcf4162b","x":-2717,"y":1578,"width":399,"height":111},
		{"type":"text","text":"## Vectorization\nWhen handling a number of computations, like in Multiple Linear Regression where a number of multiplications is to be done; instead of using indexing (in for loops) to calculate the value of \n$$\\begin{align}\nf_{W,b}(X)=W\\cdot X + b \\\\\nf_{W,b}(X)= \\left( \\sum^{n}_{j=1} w_j x_j \\right) + b\n\\end{align}$$\nwe can use vectorization in *python code* to implement parallel computation of arithmetic operations for all $j$ simultaneosly.\n\nThis can be done using `numpy.dot(W,X)` function in Numpy.\n\n","id":"05af23aaea7a20f7","x":242,"y":1390,"width":481,"height":490},
		{"type":"file","file":"Supervised Machine Learning.md","id":"1c4143ed866ab973","x":707,"y":-130,"width":449,"height":374},
		{"type":"file","file":"Supervised Machine Learning.md","subpath":"#Classification","id":"36d6caffca6a8dbb","x":1400,"y":-580,"width":449,"height":374},
		{"type":"text","text":"# different methods of classification\n\n- ### Logistic Regression\n- ### K-Nearest Neighbours\n- ### others\n","id":"496d02c68a5c4da9","x":2120,"y":-760,"width":400,"height":307},
		{"type":"file","file":"cost function.md","id":"54f6685e9a74b7e7","x":3080,"y":595,"width":574,"height":400},
		{"type":"file","file":"Simple Linear Regression.md","id":"35afa7953c9f73f1","x":2127,"y":995,"width":539,"height":406},
		{"type":"text","text":"![[gradient descent#types of gradient descent]]","id":"993f8ab72d7b3cfb","x":3580,"y":1401,"width":501,"height":377},
		{"type":"file","file":"gradient descent.md","subpath":"#gradient descent for simple linear regression","id":"4c0c49f93c4ee8d6","x":1920,"y":1778,"width":605,"height":520},
		{"type":"file","file":"gradient descent.md","id":"c0bdf6dcf747938d","x":2801,"y":1500,"width":527,"height":488},
		{"type":"file","file":"screenshots/Pasted image 20230312144702.png","id":"15d224b9fb475c34","x":2080,"y":-100,"width":759,"height":317},
		{"type":"file","file":"Logistic Regression.md","id":"59a338dfada827d5","x":2920,"y":-1073,"width":679,"height":620},
		{"type":"file","file":"Logistic Regression.md","subpath":"#cost function for logistic regression","id":"c3f11a7d714f6d35","x":3080,"y":-206,"width":665,"height":538},
		{"type":"file","file":"screenshots/Pasted image 20230312131823.png","id":"e13c667c9d042035","x":4080,"y":-1036,"width":650,"height":430}
	],
	"edges":[
		{"id":"6831132d061238f6","fromNode":"fcbd7b06fa2b0ee2","fromSide":"top","toNode":"956ae810b046fc5c","toSide":"left"},
		{"id":"f206f33a6d762d71","fromNode":"fcbd7b06fa2b0ee2","fromSide":"bottom","toNode":"22e1a2f895acf05e","toSide":"top"},
		{"id":"ece1281283a3f6ce","fromNode":"22e1a2f895acf05e","fromSide":"right","toNode":"1c4143ed866ab973","toSide":"left"},
		{"id":"44ccb1ece13d1b33","fromNode":"0176eef9c6394291","fromSide":"bottom","toNode":"35afa7953c9f73f1","toSide":"top","label":"one variable"},
		{"id":"5fed454c48a0158e","fromNode":"35afa7953c9f73f1","fromSide":"right","toNode":"54f6685e9a74b7e7","toSide":"left","label":"driven by"},
		{"id":"1eaf908bf4d230dc","fromNode":"54f6685e9a74b7e7","fromSide":"bottom","toNode":"c0bdf6dcf747938d","toSide":"top","label":"works with"},
		{"id":"9116bb401b7c500d","fromNode":"1c4143ed866ab973","fromSide":"bottom","toNode":"973ee40c93fc1cd9","toSide":"left","label":"problem type 1"},
		{"id":"7eb7a4794cfbf428","fromNode":"c0bdf6dcf747938d","fromSide":"right","toNode":"993f8ab72d7b3cfb","toSide":"left"},
		{"id":"42d4614159891050","fromNode":"0176eef9c6394291","fromSide":"bottom","toNode":"0cbc1ee4808852ff","toSide":"top","label":"multiple\nvariables"},
		{"id":"9dffac325866f186","fromNode":"973ee40c93fc1cd9","fromSide":"bottom","toNode":"0176eef9c6394291","toSide":"top"},
		{"id":"3ed03c03405f0359","fromNode":"973ee40c93fc1cd9","fromSide":"bottom","toNode":"476a43ca8fb26d53","toSide":"left"},
		{"id":"a200bc3c41a65c48","fromNode":"05af23aaea7a20f7","fromSide":"right","toNode":"0cbc1ee4808852ff","toSide":"left","label":"used in"},
		{"id":"9120a95037281268","fromNode":"0cbc1ee4808852ff","fromSide":"bottom","toNode":"8ba726aeedb83a04","toSide":"top"},
		{"id":"16d7ed0d4ec41f7a","fromNode":"c0bdf6dcf747938d","fromSide":"left","toNode":"4c0c49f93c4ee8d6","toSide":"right"},
		{"id":"141343b304b12ae7","fromNode":"4c0c49f93c4ee8d6","fromSide":"top","toNode":"35afa7953c9f73f1","toSide":"bottom"},
		{"id":"56aa0a9bbebb6b90","fromNode":"72ebb3e3d6b2058b","fromSide":"left","toNode":"a9a6a839bcf4162b","toSide":"right","label":"unscaled"},
		{"id":"e736129c5c3acf8e","fromNode":"45f22145efd1381c","fromSide":"left","toNode":"b46217417c022682","toSide":"right","label":"scaled"},
		{"id":"a4f7f93a665858dc","fromNode":"72ebb3e3d6b2058b","fromSide":"bottom","toNode":"45f22145efd1381c","toSide":"top"},
		{"id":"d88391f920b4489a","fromNode":"45f22145efd1381c","fromSide":"right","toNode":"e5b14976b0d7ed55","toSide":"left","label":"implementation"},
		{"id":"62b3e2f353a425e1","fromNode":"e5b14976b0d7ed55","fromSide":"bottom","toNode":"9209659129726005","toSide":"top","label":"output"},
		{"id":"41feb0948b2da16c","fromNode":"fcbd7b06fa2b0ee2","fromSide":"left","toNode":"27c97ef539264f37","toSide":"top"},
		{"id":"aeb6591f5ada2b6f","fromNode":"27c97ef539264f37","fromSide":"bottom","toNode":"72ebb3e3d6b2058b","toSide":"top"},
		{"id":"b5b6ca8f35d8d926","fromNode":"27c97ef539264f37","fromSide":"left","toNode":"6c20ba169c6e4188","toSide":"right"},
		{"id":"a09ecdd4af2d3f89","fromNode":"c0bdf6dcf747938d","fromSide":"bottom","toNode":"9e980a31b13e06b3","toSide":"top"},
		{"id":"062aec8dcd7ce26d","fromNode":"1c4143ed866ab973","fromSide":"top","toNode":"36d6caffca6a8dbb","toSide":"left","label":"problem type 2"},
		{"id":"cc1b5c499769b398","fromNode":"36d6caffca6a8dbb","fromSide":"right","toNode":"496d02c68a5c4da9","toSide":"left"},
		{"id":"2b2e3ec91c0de4ac","fromNode":"496d02c68a5c4da9","fromSide":"right","toNode":"59a338dfada827d5","toSide":"left"},
		{"id":"db0906aee1e87df5","fromNode":"59a338dfada827d5","fromSide":"right","toNode":"e13c667c9d042035","toSide":"left","label":"sigmoid function and \ncorresponding \ndecision boundary"},
		{"id":"d5aa60c887046c76","fromNode":"59a338dfada827d5","fromSide":"bottom","toNode":"c3f11a7d714f6d35","toSide":"top"},
		{"id":"1919faffa0d2aa2a","fromNode":"c3f11a7d714f6d35","fromSide":"left","toNode":"15d224b9fb475c34","toSide":"right"}
	]
}