{
	"nodes":[
		{"type":"group","id":"daa24c02798845c9","x":-2614,"y":-837,"width":2048,"height":1585,"label":"calculation of activation of neurons"},
		{"type":"group","id":"7aab876978ddd8b4","x":1266,"y":-1067,"width":520,"height":460,"label":"example"},
		{"type":"text","text":"## calculation of weighted sum *$z$* (for system of node)\n\nfor a system of\n- 2 neurons in *input layer*\n- 3 neurons in *current layer*\n\nwe have\n\n$$\n\\mathbf{W} = \\begin{bmatrix}\nw_{11} & w_{12} & w_{13} \\\\\nw_{21} & w_{22} & w_{23} \\\\\n\\end{bmatrix}\n\\hspace{1cm}\n\\vec {\\mathbf{x}} = \\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\n\\end{bmatrix}\n\\hspace{1cm}\n\\vec {\\mathbf{z}} = \\begin{bmatrix}\nz_1 \\\\\nz_2 \\\\\nz_3 \\\\\n\\end{bmatrix}\n$$\n$i$ = neuron$_i$ in current layer = $z_i$ ($z$ represents weighted sum)\n$j$ = input$_j$ = $x_j$ \n\nwhere\n- in $\\mathbf{W}$\n\t- $i^{th}$ row, i.e. $[w_{i1}, w_{i2}, w_{i3}]$ represents the weights to the input $x_i$ \n\t- $j^{th}$ column, i.e. $[w_{1j}, w_{2j}]$ represents the 2 weights that are going in $z_i$ \n- in $\\vec{\\mathbf{x}}$\n\t- $i^{th}$ row, i.e. $x_i$ is the $i^{th}$ input\n- in $\\vec{\\mathbf{z}}$\n\t- $i^{th}$ row, i.e. $z_i$ is the weighted sum of inputs $i^{th}$ neuron in current layer\n\nthe weighted sum is given as:\n$$\n\\vec{\\mathbf{z}}=\n\\begin{bmatrix}\nz_1 \\\\\nz_2 \\\\\nz_3 \\\\\n\\end{bmatrix}=\n\\begin{bmatrix}\nw_{11}x_1 + w_{21}x_2 \\\\\nw_{12}x_1 + w_{22}x_2 \\\\ \nw_{13}x_1 + w_{23}x_2 \\\\\n\\end{bmatrix}\n$$\nthus, $\\vec{\\mathbf{z}}$ is obtained by dot product of $\\mathbf{W}$ and $\\vec{\\mathbf{x}}$ or matrix multiplication of $\\mathbf{W}^T$ and $\\vec{\\mathbf{x}}$, i.e.\n$$\\vec{\\mathbf{z}} = \\mathbf{W} \\cdot \\vec{\\mathbf{x}} = \\mathbf{W}^T \\vec{\\mathbf{x}}$$\n$$\n\\vec {\\mathbf{z}} = \\begin{bmatrix}\nz_1 \\\\\nz_2 \\\\\nz_3 \\\\\n\\end{bmatrix}=\n\\mathbf{W}^T \\vec {\\mathbf{x}} = \\begin{bmatrix}\nw_{11} & w_{21} \\\\\nw_{12} & w_{22} \\\\\nw_{13} & w_{23} \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\n\\end{bmatrix}=\n\\begin{bmatrix}\nw_{11}x_1 + w_{21}x_2 \\\\\nw_{12}x_1 + w_{22}x_2 \\\\ \nw_{13}x_1 + w_{23}x_2 \\\\\n\\end{bmatrix}=\n\\vec{\\mathbf{z}}=\\begin{bmatrix}\nz_1 \\\\\nz_2 \\\\\nz_3 \\\\\n\\end{bmatrix}\n$$","id":"44e167dd408da41a","x":-2594,"y":-297,"width":700,"height":1025},
		{"type":"text","text":"The weight matrix can be denoted as vector of column vectors\n$$\n\\mathbf{W} = \\begin{bmatrix}\nw_{11} & w_{12} & w_{13} \\\\\nw_{21} & w_{22} & w_{23} \\\\\n\\end{bmatrix} =\n[\\vec{\\mathbf{w}}_1, \\vec{\\mathbf{w}}_2, \\vec{\\mathbf{w}}_3]\n$$\nwhere\n$$\n\\vec{\\mathbf{w}}_j = \n\\begin{bmatrix}\nw_{1j} \\\\\nw_{2j} \\\\\n\\end{bmatrix}\n$$\nis the weight associated to $z_j$, i.e.\n$$z_j = w_{1j}x_1 + w_{2j}x_2$$ ","id":"c0f1eda8539590ca","x":-2352,"y":-761,"width":498,"height":364},
		{"type":"text","text":"## calculation of activation *$a$* (for a system of nodes)\n\nevaluate the activation function of weighted sum vector $\\vec{\\mathbf{z}}$ after adding a bias $b$\n$$\\vec{\\mathbf{a}} = f(\\vec{\\mathbf{z}}+b)$$\n$$\n\\vec{\\mathbf{a}} = \n\\begin{bmatrix}\na_1 \\\\\na_2 \\\\\na_3 \\\\\n\\end{bmatrix} =\n\\begin{bmatrix}\nf(z_1 + b) \\\\\nf(z_2 + b) \\\\\nf(z_3 + b) \\\\\n\\end{bmatrix} =f\n\\left(\n\\begin{bmatrix}\nz_1 \\\\\nz_2 \\\\\nz_3 \\\\\n\\end{bmatrix}\n+ b\n\\right)\n$$","id":"42e36837b844af27","x":-1612,"y":-323,"width":576,"height":341},
		{"type":"text","text":"## calculation of activation *$a$* (for one node)\n\nfor a neuron getting $n$ inputs (from previous layer),\n- $\\vec {\\mathbf w}$: weights associated to inputs\n- $\\vec {\\mathbf x}$: input vector\n\n$$\n\\vec {\\mathbf{w}} = \\begin{bmatrix}\nw_1 \\\\\nw_2 \\\\\n\\vdots \\\\\nw_n\n\\end{bmatrix}\n\\hspace{1cm}\n\\vec {\\mathbf{x}} = \\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\n\\vdots \\\\\nx_n\n\\end{bmatrix}\n$$\nthe activation $a$ is determined as:\n$$\n\\begin{align}\na &= f\\left(\\sum_{i=1}^{n}w_ix_i + b \\right) \\\\\na &= f\\left(\\vec{\\mathbf{w}} \\cdot \\vec{\\mathbf{x}} + b \\right)\n\\end{align}\n$$\nwhere, $\\vec{\\mathbf{w}} \\cdot \\vec{\\mathbf{x}}$ acts as the weighted sum of inputs\n$$\n\\begin{align}\n\\vec{\\mathbf{w}} \\cdot \\vec{\\mathbf{x}} &= [w_1x_1+w_2x_2+ \\cdots+w_nx_n] \\\\\n&= \\sum_{i=1}^{n}w_ix_i\n\\end{align}\n$$\n","id":"5bc902678797c190","x":-1736,"y":88,"width":700,"height":630},
		{"type":"text","text":"Since\n$$\n\\vec{\\mathbf{w}}_j=\n\\begin{bmatrix}\n\tw_{j1} \\\\\n\tw_{j2} \\\\\n\\end{bmatrix}, \\hspace{1cm}\n\\vec{\\mathbf{x}}=\n\\begin{bmatrix}\n\tx_1 \\\\\n\tx_2 \\\\\n\\end{bmatrix}\n$$\nwe can have\n$$\nz_j = w_{1j}x_1 + w_{2j}x_2 = \\vec{\\mathbf{w}}_j \\cdot \\vec{\\mathbf{x}}\n$$\nand\n$$\n\\vec{\\mathbf{z}} = \n\\begin{bmatrix}\nz_1 \\\\\nz_2 \\\\\nz_3 \\\\\n\\end{bmatrix} = \n\\begin{bmatrix}\nw_{11}x_1 + w_{21}x_2 \\\\\nw_{12}x_1 + w_{22}x_2 \\\\\nw_{13}x_1 + w_{23}x_2 \\\\\n\\end{bmatrix} = \n\\begin{bmatrix}\n\\vec{\\mathbf{w}}_1 \\cdot \\vec{\\mathbf{x}} \\\\\n\\vec{\\mathbf{w}}_2 \\cdot \\vec{\\mathbf{x}} \\\\\n\\vec{\\mathbf{w}}_3 \\cdot \\vec{\\mathbf{x}} \\\\\n\\end{bmatrix}\n$$\n\n\n","id":"8c0d232ed434a5da","x":-1736,"y":-797,"width":478,"height":400},
		{"type":"text","text":"$a^{[1]}$","id":"cc567d1a47e91276","x":1406,"y":-695,"width":80,"height":50},
		{"type":"file","file":"screenshots/Pasted image 20230604025315.png","id":"e727fdf8c2d82f87","x":1286,"y":-1027,"width":469,"height":320},
		{"type":"text","text":"$\\vec{\\mathbf{x}}$","id":"605a6bba302549e0","x":1306,"y":-695,"width":57,"height":50},
		{"type":"text","text":"$a^{[3]}$","id":"739e3ead4e355184","x":1686,"y":-695,"width":80,"height":50},
		{"type":"text","text":"$a^{[2]}$","id":"7d0b5fc9eaff6766","x":1546,"y":-695,"width":80,"height":50},
		{"type":"text","text":"<div style=\"font-family:Latin Modern Math; font-size:70;\">Deep Learning</divv>\n\nA *branch* (or rather an implementation of) of *Artificial Intelligence* which is concerned with mimicking the working of the Human Brain Cell **Neuron**\n\nDeep learning concerns with **Artificial Neural Networks** (and its various types), made of **Artificial Neurons**. \n\n## advantages\n- automatic feature selection and engineering from a vector of inputs","id":"e1bc8d24346b7241","x":-70,"y":1393,"width":536,"height":440},
		{"type":"text","text":"## calculation of *$a^{[1]}$*\n\n$$\n\\vec{\\mathbf{a}}^{[1]} = f(\\mathbf{W}^{[1]} \\cdot \\vec{\\mathbf{x}} + \\vec{\\mathbf{b}}^{[1]})\n$$\n\n$$\n\\vec{\\mathbf{a}}^{[1]} = \n\\begin{bmatrix}\na_1^{[1]} \\\\\na_2^{[1]} \\\\\na_3^{[1]} \\\\\na_4^{[1]} \\\\\n\\end{bmatrix} = \n\\begin{bmatrix}\nf(\\vec{\\mathbf{w}}^{[1]}_1 \\cdot \\vec{\\mathbf{x}} + b_1^{[1]}) \\\\\nf(\\vec{\\mathbf{w}}^{[1]}_2 \\cdot \\vec{\\mathbf{x}} + b_2^{[1]}) \\\\\nf(\\vec{\\mathbf{w}}^{[1]}_3 \\cdot \\vec{\\mathbf{x}} + b_3^{[1]}) \\\\\nf(\\vec{\\mathbf{w}}^{[1]}_4 \\cdot \\vec{\\mathbf{x}} + b_4^{[1]}) \\\\\n\\end{bmatrix}\n$$","id":"3067225b99eddcda","x":370,"y":-410,"width":393,"height":286},
		{"type":"text","text":"# Artificial Neuron\n\n- It is the building block of a Neural Network\n- It receives inputs from multiple neurons and gives out a single output\n- that output is then sent to multiple neurons \n\nThe output of a neuron is called *activation* and is given by\n$$a = f\\left(\\sum_{i=1}^{n}w_ix_i+b\\right)=f(\\vec{\\mathbf{w}}\\cdot \\vec{\\mathbf{b}}+c)=f(z)$$\nfor activation function $f(\\cdot)$.\n\n$w_i$ represents the weight associated to input $x_i$.","id":"53cd9cff70059120","x":-274,"y":323,"width":540,"height":480},
		{"type":"text","text":"# Neural Network Layers\n\nThe neural networks typically use a layer of such neurons between the input layer and the output layer. These are called the ***Hidden Layers***.\n\nThe output from one layer becomes the input to the next layer.","id":"110cff2ff4050951","x":-358,"y":-528,"width":442,"height":263},
		{"type":"text","text":"# intuition for Neural Networks\n\nwhen creating a *Machine Learning* prediction model, suppose for binary classification with *logistic regression*, we use the model \n$$f(x)_{W,b} = \\frac{1}{1+e^{wx+b}}$$\nthis can ","id":"c6d7d61a640066c8","x":466,"y":803,"width":380,"height":447},
		{"type":"text","text":"# Activation Functions\n\nthe are the functions which give the *required transformation* on the $z_j = w_{1j}x_1 + w_{2j}x_2$, i.e.\n$$a = f(z)$$\nThe linear transformation obtained from weighted sum can only model a linear relationship between inputs and outputs. Real world problems are much more complex and are non-linear in nature. \n\nActivation functions add the non-linearity required to the decision making process (forward propagation). It helps understand / represent complex patterns and relationships in data.\n\n","id":"f8d68adc54d9fbe8","x":567,"y":108,"width":612,"height":420},
		{"type":"text","text":"## calculation of *$a^{[2]}$*\n\n$$\n\\vec{\\mathbf{a}}^{[2]} = f(\\mathbf{W}^{[2]} \\cdot \\vec{\\mathbf{a}}^{[1]} + \\vec{\\mathbf{b}}^{[2]})\n$$\n\n$$\n\\vec{\\mathbf{a}}^{[2]} = \n\\begin{bmatrix}\na_1^{[2]} \\\\\na_2^{[2]} \\\\\na_3^{[2]} \\\\\n\\end{bmatrix} = \n\\begin{bmatrix}\nf(\\vec{\\mathbf{w}}^{[2]}_1 \\cdot \\vec{\\mathbf{a}}^{[1]} + b_1^{[2]}) \\\\\nf(\\vec{\\mathbf{w}}^{[2]}_2 \\cdot \\vec{\\mathbf{a}}^{[1]} + b_2^{[2]}) \\\\\nf(\\vec{\\mathbf{w}}^{[2]}_3 \\cdot \\vec{\\mathbf{a}}^{[1]} + b_3^{[2]}) \\\\\n\\end{bmatrix}\n$$","id":"8edbd5d49edfff7b","x":806,"y":-410,"width":393,"height":243},
		{"type":"text","text":"## calculation of weighted sum *$z$* (for a system of layers)\n\nimagine a neural network with\n- *2 neurons* in input layer (layer 0)\n- *4 neurons* in layer 1\n- *3 neurons* in layer 2\n- *1 neuron* in output layer (layer 3)\n\nThe output of layer 1, 2 and 3 are $a^{[1]}$, $a^{[2]}$ and $a^{[3]}$, respectively","id":"d1b984c9a42ea588","x":370,"y":-1012,"width":560,"height":342},
		{"type":"text","text":"# to do\n\n- [ ] computation graph\n- [ ] Backpropagation","id":"14f277a15e92b08d","x":2242,"y":-692,"width":373,"height":427},
		{"type":"text","text":"## sigmoid vs. softmax\n\nhttps://web.stanford.edu/~nanbhas/blog/sigmoid-softmax/","id":"c4bc4cdc11f8f66d","x":1986,"y":-37,"width":256,"height":246},
		{"type":"text","text":"## Commonly used Activation Functions\n- **Linear Function** \n\t- $$f(x) = x$$\n\t- same value as input\n- **Sigmoid Function** \n\t- $$f(x) = \\frac{1}{1+e^{-x}}$$\n\t- map the value between 0 and 1\n- **Tanh Function** \n\t- $$f(x) = \\frac{e^x-e^{-x}}{e^x+e^{-x}}$$\n\t- map the value between -1 and 1\n\n- **Softmax Function** \n\t- $$f(\\vec{\\mathbf{z}})_i = \\frac{e^{z_i}}{\\sum_{j=1}^{K}e^{z_j}}$$\n\t- apply logistic function on multiple dimensions\n- **ReLU Function** \n\t- $$f(x) = max(0,x) = \\begin{cases} x & x \\ge 0 \\\\ 0 & x < 0 \\end{cases}$$\n\t- faster, vanishing gradient (in above two)\n- **Leaky ReLU Function** \n\t- $$f(x) = max(0,x) = \\begin{cases} x & x \\ge 0 \\\\ \\alpha x & x < 0 \\end{cases}$$\n\t- solves dying gradient","id":"701c750838d4e985","x":1374,"y":-167,"width":492,"height":970},
		{"type":"file","file":"screenshots/tf_layers.pdf","id":"656a3b5df0cfd49e","x":-1258,"y":-1832,"width":900,"height":820},
		{"type":"text","text":"## inputs to a neuron\n\n- They are sent as a vector $\\vec {\\mathbf{x}}$ consisting of $x_i$ elements","id":"edf888125dc4d492","x":-914,"y":-22,"width":328,"height":141},
		{"type":"text","text":"## weights to inputs\n\n- these are associated to each input to a neuron\n- represents the weight/importance attached to the input\n- represented as vector $\\vec{\\mathbf w}$ consisting of weight $w_i$ corresponding to input $x_i$.","id":"68d9af78a7eaf4e2","x":-914,"y":158,"width":328,"height":320},
		{"type":"text","text":"## bias\n- added to each neuron to shift the product of weights and inputs linearly","id":"704215eeab704656","x":-914,"y":508,"width":328,"height":200},
		{"type":"text","text":"## calculation of *$a^{[3]}$*\n\n$$\n\\vec{\\mathbf{a}}^{[3]} = f(\\mathbf{W}^{[2]} \\cdot \\vec{\\mathbf{a}}^{[2]} + \\vec{\\mathbf{b}}^{[3]})\n$$\n\n$$\n\\vec{\\mathbf{a}}^{[3]} = \n\\begin{bmatrix}\na_1^{[3]} \\\\\n\\end{bmatrix} = \n\\begin{bmatrix}\nf(\\vec{\\mathbf{w}}^{[3]}_1 \\cdot \\vec{\\mathbf{a}}^{[2]} + b_1^{[3]}) \\\\\n\\end{bmatrix}\n$$","id":"dd704bb0cac55c77","x":1250,"y":-410,"width":393,"height":180},
		{"type":"link","url":"https://www.youtube.com/watch?v=QDX-1M5Nj7s&list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI&pp=iAQB","id":"b96c69d7bd849f40","x":-3920,"y":703,"width":1050,"height":690}
	],
	"edges":[
		{"id":"c72c0790f87c7c9d","fromNode":"53cd9cff70059120","fromSide":"left","toNode":"daa24c02798845c9","toSide":"right","label":"calculation"},
		{"id":"a013c74fed439941","fromNode":"d1b984c9a42ea588","fromSide":"right","toNode":"7aab876978ddd8b4","toSide":"left"},
		{"id":"121a5480b6221d59","fromNode":"d1b984c9a42ea588","fromSide":"bottom","toNode":"3067225b99eddcda","toSide":"top"},
		{"id":"6dc80944cffe2d9d","fromNode":"d1b984c9a42ea588","fromSide":"bottom","toNode":"8edbd5d49edfff7b","toSide":"top"},
		{"id":"3d28c85e540e28a0","fromNode":"d1b984c9a42ea588","fromSide":"bottom","toNode":"dd704bb0cac55c77","toSide":"top"},
		{"id":"9da86f36c56c650f","fromNode":"110cff2ff4050951","fromSide":"right","toNode":"d1b984c9a42ea588","toSide":"left"},
		{"id":"b2584e0a44a01189","fromNode":"f8d68adc54d9fbe8","fromSide":"right","toNode":"701c750838d4e985","toSide":"left"},
		{"id":"0f6ff0317a4d25d4","fromNode":"53cd9cff70059120","fromSide":"right","toNode":"f8d68adc54d9fbe8","toSide":"left","label":"f(z)"},
		{"id":"10a552dc61f7ea14","fromNode":"701c750838d4e985","fromSide":"right","toNode":"c4bc4cdc11f8f66d","toSide":"left"},
		{"id":"e742a52a155b4e30","fromNode":"44e167dd408da41a","fromSide":"right","toNode":"42e36837b844af27","toSide":"left"},
		{"id":"df28265e2f8d3876","fromNode":"44e167dd408da41a","fromSide":"top","toNode":"c0f1eda8539590ca","toSide":"bottom"},
		{"id":"16767624c2191f60","fromNode":"c0f1eda8539590ca","fromSide":"right","toNode":"8c0d232ed434a5da","toSide":"left"},
		{"id":"34346cd296c95001","fromNode":"53cd9cff70059120","fromSide":"top","toNode":"110cff2ff4050951","toSide":"bottom","label":"layers"},
		{"id":"70b3aaad322bbbe6","fromNode":"e1bc8d24346b7241","fromSide":"top","toNode":"53cd9cff70059120","toSide":"bottom"},
		{"id":"2a10b6f75ad5f0d5","fromNode":"110cff2ff4050951","fromSide":"top","toNode":"656a3b5df0cfd49e","toSide":"bottom"},
		{"id":"fdf6e62badd5df8e","fromNode":"8c0d232ed434a5da","fromSide":"bottom","toNode":"42e36837b844af27","toSide":"top"},
		{"id":"da6413f116e4d4b7","fromNode":"edf888125dc4d492","fromSide":"left","toNode":"5bc902678797c190","toSide":"right"},
		{"id":"35e043f4a86a3434","fromNode":"68d9af78a7eaf4e2","fromSide":"left","toNode":"5bc902678797c190","toSide":"right"},
		{"id":"8a54b41f84338f54","fromNode":"704215eeab704656","fromSide":"left","toNode":"5bc902678797c190","toSide":"right"},
		{"id":"2e92abc79d71fc04","fromNode":"e1bc8d24346b7241","fromSide":"top","toNode":"c6d7d61a640066c8","toSide":"left"}
	]
}