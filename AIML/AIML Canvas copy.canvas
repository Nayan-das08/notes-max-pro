{
	"nodes":[
		{"type":"group","id":"81aac4003fc70a7a","x":2720,"y":2400,"width":2663,"height":864,"label":"Blueprint"},
		{"type":"group","id":"ac54b113c1317a39","x":-1360,"y":3520,"width":760,"height":378,"label":"workflow for bias variance handling in Neural Networks"},
		{"type":"text","text":"## Deep Learning","id":"9c3f695211aaaf8b","x":3300,"y":2420,"width":221,"height":105},
		{"type":"text","text":"## improving models\n### Feature Scaling, Feature Selection, Feature Engg, Overfitting, regularization","id":"c4fb5b8454fcf909","x":2740,"y":2587,"width":300,"height":211},
		{"type":"text","text":"# Machine Learning","id":"0b587edb2dad7652","x":3240,"y":2731,"width":341,"height":195},
		{"type":"text","text":"## Supervised","id":"7f9c8775eb9fae08","x":3709,"y":2776,"width":221,"height":105},
		{"type":"text","text":"## Other algo\n### XG-Boost, SVM, Decision Tree, Random Forest, Naiive Bayse","id":"56669d8ee483f171","x":3967,"y":2974,"width":309,"height":169},
		{"type":"text","text":"## unsupervised","id":"f7beeb7d7c5ca525","x":3289,"y":3075,"width":244,"height":169},
		{"type":"text","text":"## model evaluation\n### cross-validation, errors, bias-variance","id":"0995bd2cea90de16","x":2764,"y":2895,"width":263,"height":157},
		{"type":"file","file":"AIML/Machine Learning.md","id":"22e1a2f895acf05e","x":1760,"y":840,"width":760,"height":520},
		{"type":"text","text":"## regression problem","id":"f25addc893353a68","x":4121,"y":2529,"width":249,"height":122},
		{"type":"text","text":"## simple","id":"1a9028364ece0f59","x":4715,"y":2480,"width":250,"height":60},
		{"type":"text","text":"## multiple","id":"fb71847f75f136be","x":4715,"y":2591,"width":250,"height":60},
		{"type":"text","text":"## logistic","id":"ca34c19bf1dd01c5","x":4718,"y":2680,"width":250,"height":60},
		{"type":"file","file":"AIML/Supervised Machine Learning.md","id":"1c4143ed866ab973","x":2920,"y":840,"width":560,"height":520},
		{"type":"file","file":"AIML/Supervised Machine Learning.md","subpath":"#Classification","id":"dc30a3ff2502f2be","x":4160,"y":1240,"width":480,"height":507},
		{"type":"text","text":"### Naiive Bayes","id":"b633c59ee9509b72","x":4743,"y":3143,"width":128,"height":85},
		{"type":"text","text":"<h1 style=\"font-size:70;\">Unsupervised ML</h1>","id":"b9cc88785fc0fc15","x":2000,"y":1680,"width":706,"height":171},
		{"type":"text","text":"## classification problem","id":"2581eecef4ff5ef6","x":4121,"y":2702,"width":249,"height":122},
		{"type":"text","text":"### grad descent and cost function","id":"4bdeacbd00e16b59","x":5083,"y":2510,"width":250,"height":97},
		{"type":"file","file":"AIML/Simple Linear Regression.md","id":"808b1d56f2480d41","x":4914,"y":834,"width":539,"height":406},
		{"type":"text","text":"# different methods of classification\n\n- ### Logistic Regression\n- ### K-Nearest Neighbours\n- ### XG-Boost\n- ### Decision Tree\n","id":"09152c6f9e14d174","x":4951,"y":1594,"width":400,"height":307},
		{"type":"text","text":"### SVM","id":"a06f23b7a7ca66ce","x":4743,"y":2966,"width":148,"height":50},
		{"type":"text","text":"### KNN","id":"96cd2ff4029651ac","x":4743,"y":3059,"width":195,"height":50},
		{"type":"text","text":"### XG-Boost","id":"feaf902c3a35a109","x":4739,"y":2779,"width":195,"height":50},
		{"type":"text","text":"![[Supervised Machine Learning#^5450a3]]","id":"6c63638f6b152700","x":4720,"y":280,"width":387,"height":186},
		{"type":"text","text":"## Vectorization\nWhen handling a number of computations, like in Multiple Linear Regression where a number of multiplications is to be done; instead of using indexing (in for loops) to calculate the value of \n$$\\begin{align}\nf_{W,b}(X)=W\\cdot X + b \\\\\nf_{W,b}(X)= \\left( \\sum^{n}_{j=1} w_j x_j \\right) + b\n\\end{align}$$\nwe can use vectorization in *python code* to implement parallel computation of arithmetic operations for all $j$ simultaneosly.\n\nThis can be done using `numpy.dot(W,X)` function in Numpy.\n\n","id":"fa6f62713f039a5e","x":4867,"y":-1081,"width":481,"height":490},
		{"type":"file","file":"AIML/Multiple Linear Regression.md","id":"410ec58058cb7e41","x":5108,"y":-414,"width":660,"height":474},
		{"type":"text","text":"![[Supervised Machine Learning#^376c22]]","id":"476a43ca8fb26d53","x":4000,"y":-100,"width":459,"height":160},
		{"type":"file","file":"AIML/Supervised Machine Learning.md","subpath":"#Regression","id":"973ee40c93fc1cd9","x":4000,"y":466,"width":449,"height":374},
		{"type":"file","file":"Deep Learning.canvas","id":"8d6165954ab862a8","x":1419,"y":-591,"width":877,"height":400},
		{"type":"text","text":"<div style=\"font-family:Latin Modern Math; font-size:70;\">Deep Learning</divv>\n\nA *branch* (or rather an implementation of) of *Artificial Intelligence* which is concerned with mimicking the working of the Human Brain Cell **Neuron**\n\nDeep learning concerns with **Artificial Neural Networks** (and its various types), made of **Artificial Neurons**. \n\n## advantages\n- automatic feature selection and engineering from a vector of inputs","id":"dc7c01494986628a","x":1732,"y":153,"width":536,"height":440},
		{"type":"text","text":"## establishing a baseline level of performance\n\n>level of error that is acceptable for a particular problem\n\n- can be obtained/estimated using\n\t- human level performance\n\t- competing algorithms performance\n\t- guess based on experience","id":"8a56595189133be2","x":-976,"y":2939,"width":377,"height":370},
		{"type":"text","text":"# Bias and Variance\n\n- model has **high bias when underfit**\n- model has **high variance when overfit**\n---\n- $J_{train} \\uparrow$, $J_{CV} \\uparrow$ $\\Rightarrow$ *high bias*\n- $J_{train} \\downarrow$, $J_{CV} \\uparrow$ $\\Rightarrow$ *high variance*\n- $J_{train} \\downarrow$, $J_{CV} \\downarrow$ $\\Rightarrow$ *best fit*\n- $J_{train} \\uparrow$, $J_{CV} >> J_{train}$ $\\Rightarrow$ *high bias and high variance*","id":"6e993623904444e7","x":-323,"y":3109,"width":449,"height":369},
		{"type":"text","text":"# Model Selection\n\none way of selecting the models for a single problem, where the models vary in complexity (eg: degree of polynomial of the hypothesis function $f_{\\vec{\\mathbf{w}},b}(\\vec{\\mathbf{x}})$) is done by comparing the *result of the model on test dataset*.\n\n**process**\n- decide model\n- fit the model (train) on training set\n- get parameters $\\vec{\\mathbf{w}}$ and $b$ \n- test the model on testing set and get $J_{test}(\\vec{\\mathbf{w}},b)$ ","id":"e020b15b735b7492","x":-63,"y":2509,"width":500,"height":400},
		{"type":"text","text":"## cross validation set\n- data points used for model selection purposes\n\t- give performance in each accuracy \n\t- give performance of each model\n- these are different from training set and thus are new to the model\n- these are different from the testing set and thus the testing result will give the **fair estimate of the generalization error**","id":"23f4e8e6eab3ade5","x":257,"y":3109,"width":341,"height":496},
		{"type":"text","text":"- the degree of polynomial of the **hypothesis function** becomes a parameter that is set on the *testing set*\n- $J_{test}(\\vec{\\mathbf{w}},b)$ is *overly optimistic estimate of generalization error on **testing set** *","id":"f83752f520ee7fb2","x":630,"y":2552,"width":407,"height":197},
		{"type":"text","text":"- when number of samples increases, the model has *more training loss* as it tries to fit over a large number of points\n- when number of samples increases, the *cross-validation loss decreases* as the model will be better trained on the training samples, thus will perform better on the cross-validation set","id":"21869d20c11f1d42","x":630,"y":3700,"width":431,"height":274},
		{"type":"file","file":"screenshots/Pasted image 20230710012756.png","id":"5e04729becceff97","x":786,"y":3109,"width":400,"height":400},
		{"type":"text","text":"- try additional features\n- try adding polynomial features (*feature engineering*)\n- decrease regularization term $\\lambda$","id":"7cbb0616917170e2","x":-423,"y":3756,"width":272,"height":233},
		{"type":"text","text":"- get more training samples\n- try smaller set of features\n- increase regularization term $\\lambda$\n","id":"0c6ad714dce8ce60","x":-43,"y":3756,"width":280,"height":233},
		{"type":"text","text":"<div style=\"font-family:Latin Modern Math; font-size:70;\">Model Evaluation</div>\n\nThe models have to be evaluated for their accuracy. This is done using *specific loss functions* on the available dataset. \n\nFor example\n- **Mean Squared Error (MSE)**\n\t- used for regression problems\n\t- $$J(\\vec{\\mathbf{w}},b)=\\frac{1}{2m}\\sum^{m}_{i=1}(\\hat{y}^{(i)}-{y}^{(i)})^2$$\n- **Binary Cross-entropy**\n\t- used for binary classification\n\t- $$J(W,b)=-\\frac{1}{m}\\sum_{i=1}^{m}(y^{(i)}\\cdot \\ln(\\hat{y}^{(i)}))+((1-y^{(i)})\\cdot \\ln(1-\\hat{y}^{(i)}))$$\n- **Categorical Cross-entropy**\n\t- used for multi-class classification\n\t- $$J(\\vec{\\mathbf{w}},b) = -\\sum_{i=1}^{m} y^{(i)}  \\cdot \\ln(\\hat{y}^{(i)})$$","id":"ddf0d106d44106bb","x":200,"y":1503,"width":640,"height":737},
		{"type":"text","text":"# Feature Engineering\n\nUsing intuition to design *new features* by transforming or combining original features\n\nSuppose for a model,\n$$f_{W,b}(X)=w_1x_1+w_2x_2+b$$\nwhere $x_1$ is length, $x_2$ is breadth and we need to predict the price of the item using the model\n\nAnother effective way to create the model: incorporate the *area* $x_3=x_1x_2$ as well. Then the model would be\n$$f_{W,b}(X)=w_1x_1+w_2x_2+w_3x_3+b$$\n\nThis makes it easier for the model to make accurate predictions","id":"6c20ba169c6e4188","x":-397,"y":527,"width":637,"height":513},
		{"type":"text","text":"<div style=\"font-family:Latin Modern Math; text-align:center; font-size:70\">Improving Models</div>\n\n","id":"27c97ef539264f37","x":375,"y":209,"width":768,"height":190},
		{"type":"text","text":"# Feature Scaling\n\n## Why Feature Scaling is needed?\nWhen a feature of a model has a range of values that is very large (magnitude wise), the learning algorithms sets the corresponding weight very small, and vice versa.\n\nBecause of this, the 2D-contour the cost function on two features (say one which has a large range of values and another with short range) is *elliptical*.\n\nSuppose, \n- $x_1$ is the feature with large range, thus $w_1$ would be small.\n- $x_2$ is the feature with short range, thus $w_2$ would be large.\n\nTherefore,\n- A small change in $w_1$ will cause a huge change in resulting product $w_1x_1$. Thus contour will be narrow across this axis\n- A small change in $w_2$ will cause a small change in resulting product $w_2x_2$. Thus contour will be broad across this axis.\n\nSince, the cost function curve is skewed, when the model is tested, the gradient descent algo will make the weight values bounce/oscillate the narrow axis","id":"72ebb3e3d6b2058b","x":-1154,"y":542,"width":670,"height":675},
		{"type":"text","text":"## Underfitting\n- when the model *doesn't fit the data* very well $\\implies$ the model is *underfitting the data* $\\implies$ the model has *high bias*\n- eg: the given data suggests that the target is dependent on input quadratically, but the applied model is only linear\n- **bias** here refers to a preconceived notion or assumption that the data is like something else. The model is not able to capture the pattern in the data.\n- solution: **more complex model**\n\n## Best Fit\n- the model fits the data *pretty well*\n- it does not have to fit all the samples of the training set, but rather fits the data just good enough.\n- this ensures that the model will perform/predict well even on new and unseen testing samples $\\implies$ *generalization*\n\n## Overfitting\n- when the model tries to fit over all the samples of the training set, even the noise and outliers $\\implies$ the model *overfits the data* $\\implies$ the model has *high variance*.\n- this fits the data *so extremely well* that the error or cost on training samples are completely zero\n- this ends up causing problems in prediction of new test samples and would fail to generalize over new samples\n- solution: **get more training data**, **regularization**, **dropout (in DL)**, etc.","id":"bd9136c8538f1282","x":-911,"y":-420,"width":677,"height":880},
		{"type":"text","text":"## low $J_{CV}$?","id":"84b3c58bd8d74902","x":-976,"y":3676,"width":149,"height":74},
		{"type":"text","text":"## done","id":"95eeb62441dc1a15","x":-725,"y":3676,"width":109,"height":74},
		{"type":"text","text":"**Bigger Network**","id":"8e3b804d866ff08a","x":-1276,"y":3825,"width":195,"height":38},
		{"type":"text","text":"## testing set\n- split the available dataset into training and testing set\n- common ratios of splitting are *80:20* or *70:30*\n- evaluate the performance of a model on the testing set (which are new data points to the model)","id":"0f8e2298c66887e5","x":-574,"y":2429,"width":311,"height":360},
		{"type":"text","text":">[!NOTE]\n>- Once the *parameters of the model* are set to the training set, the error $J_{train}(\\vec{\\mathbf{w}}, b)$ will be low (if trained enough) \n>- this is bcoz $J_{train}(\\vec{\\mathbf{w}},b)$ is *overly optimistic estimate of generalization error on **training set** *","id":"1802be618a31ebda","x":-420,"y":1811,"width":480,"height":300},
		{"type":"text","text":"## How to perform feature scaling\nIf the values of both $x_1$ and $x_2$ are in the same/comparable range, the contour would be circular.\n\n1. **Min-Max scaling / Normalization**\n\t- maps between 0 and 1\n\t- sensitive to outliers\n\t- $$x_{new}=\\frac{x_i-min(x_i)}{max(x_i)-min(x_i)}$$\n2. Standardization / Z-score scaling\n\t- resulting data will have mean=0, std. dev.=1\n\t- less sensitive to outliers\n\t- $$x_{new}=\\frac{x_i-x_{mean}}{\\sigma}$$\n3. Robust scaling\n4. Logarithmic scaling\n5. Power scaling\n6. Unit vector scaling","id":"45f22145efd1381c","x":-2149,"y":698,"width":610,"height":669},
		{"type":"text","text":"# Solving Overfitting\n\nSome methods to address the problem of overfitting are:\n1. **increase number of training samples**\n\t- this prevents the model from being affected severely due to the noise\n\t- abundance of data is not always available\n2. **feature selection** \n\t- if we have a lot of features and insufficient data, we may encounter overfitting\n\t- if we select only a subset of relevant features to create a model we can get best fit case\n\t- feature selection leads to not so important features not being acknowledged by the model\n3. **Regularization**\n\t- in most overfit models, the weights are often very large. \n\t- regularization is a method of gently reducing the impact of some of the features without eliminating them right away ","id":"1c8696753c59742d","x":-1844,"y":-420,"width":681,"height":588},
		{"type":"file","file":"screenshots/Pasted image 20230309194213.png","id":"a9a6a839bcf4162b","x":-2086,"y":383,"width":683,"height":190},
		{"type":"text","text":"# Regularization in gradient descent\n\n## how the weights are minimized\n$$\\begin{align}\nw_j&=w_j-\\alpha\\left[\\nabla (\\text{error term}) + \\nabla (\\text{regularization term})\\right]\\\\\n&=w_j-\\alpha\\left[\\nabla (\\text{error term})+\\frac{\\lambda}{m}w_j \\right] \\\\\n&=w_j-\\alpha \\frac{\\lambda}{m}w_j-\\alpha \\nabla (\\text{error term}) \\\\\n&=w_j\\left(1-\\alpha\\frac{\\lambda}{m}\\right)-\\alpha \\nabla (\\text{error term})\n\\end{align}$$\n- here, the term $w_j\\left(1-\\alpha\\frac{\\lambda}{m}\\right)$ ensures that the $\\alpha \\nabla (\\text{error term})$ is not subtracted from $w_j$, but rather a fraction of $w_j$\n- suppose $\\lambda=1$, $m=50$ and $\\alpha=0.01$.\n- then $w_j\\left(1-\\alpha\\frac{\\lambda}{m}\\right)=(0.9998)w_j$","id":"0c9f4e0779b8cbf9","x":-3671,"y":-214,"width":580,"height":474},
		{"type":"text","text":"```\n[[0.   0. ]\n [0.5  0.5]\n [1.   1. ]]\n```","id":"9209659129726005","x":-3485,"y":626,"width":196,"height":144},
		{"type":"text","text":"## magnitude of regularization term *$\\lambda$*\n- if $\\lambda$ is too small, there will be no regularization\n\t- this leads to overfitting $\\Rightarrow$ high *variance*\n- if $\\lambda$ is too large, there weights will be very near to 0\n\t- this leads to underfitting $\\Rightarrow$ high *bias*\n- desired value is obtained using lowest $J_{CV}$ with different values of $\\lambda$ ","id":"76393e5343e2ff1c","x":-3381,"y":-594,"width":504,"height":278},
		{"type":"text","text":"# Regularization\n\n- The method of *reducing the impact of parameters* on features where the parameters have huge values and are causing **overfitting**. \n- smaller values of parameters $\\implies$ simpler model $\\implies$ less prone to overfitting\n\n## performing regularization\n- if we have many features, we may not know which features are to be penalized and reduced.\n- thus we penalize all the features\n- we penalize them by adding a new term to the cost function: a **regularization term**\n$$\\begin{gather}\nJ(W,b)=\\frac{1}{2m}\\sum_{i=1}^{m} (f_{W,b}(X)-y^{(i)})^2 + \\frac{\\lambda}{2m} \\sum_{j=1}^{n}w_j^2\\\\\\\\\n\\text{error term}=\\frac{1}{2m}\\sum_{i=1}^{m} (f_{W,b}(X)-y^{(i)})^2 \\\\\n\\text{regularization term}=\\frac{\\lambda}{2m} \\sum_{j=1}^{n}w_j^2\n\\end{gather}\n$$\n- we multiply by $\\frac{1}{2m}$ to have both the terms scaled \n- $\\lambda$ is a hyperparameter, just like $\\alpha$, the learning rate\n\t- keeping $\\lambda$ very big (say $10^{10}$) places very heavy weight on the regularization term of cost function\n\t- the model ends up minimizing all the weights very close to zero\n\t- results in *underfitting*\n\t- model needs to be checked for multiple orders of values for $\\lambda$ to determine the appropriate range","id":"993226b02f894ec9","x":-2705,"y":-420,"width":700,"height":654},
		{"type":"text","text":"```python\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\n\n# create a toy dataset\nX_train = np.array([[1., 2.],\n                    [3., 4.],\n                    [5., 6.]])\n\n# create a MinMaxScaler object and fit the training data\nscaler = MinMaxScaler()\nscaler.fit(X_train)\n\n# transform the training data using the learned parameters\nX_train_scaled = scaler.transform(X_train)\n\n# print the scaled data\nprint(X_train_scaled)\n\n```","id":"e5b14976b0d7ed55","x":-3100,"y":322,"width":617,"height":501},
		{"type":"text","text":"**get more data**","id":"f42570a0e1362ce1","x":-991,"y":3545,"width":180,"height":38},
		{"type":"text","text":"## low $J_{train}$?","id":"c443da954021fc1a","x":-1270,"y":3676,"width":182,"height":74},
		{"type":"file","file":"screenshots/Pasted image 20230312131823.png","id":"e13c667c9d042035","x":6260,"y":2271,"width":650,"height":430},
		{"type":"file","file":"AIML/Logistic Regression.md","subpath":"#cost function for logistic regression","id":"c3f11a7d714f6d35","x":6721,"y":1494,"width":665,"height":538},
		{"type":"file","file":"AIML/gradient descent.md","id":"ea13afc362a21b4d","x":6858,"y":612,"width":527,"height":488},
		{"type":"file","file":"screenshots/Pasted image 20230312144702.png","id":"15d224b9fb475c34","x":7007,"y":2271,"width":759,"height":317},
		{"type":"file","file":"AIML/Logistic Regression.md","id":"59a338dfada827d5","x":5581,"y":1614,"width":679,"height":620},
		{"type":"file","file":"AIML/cost function.md","id":"dd2b17452f547aea","x":5921,"y":960,"width":574,"height":400},
		{"type":"file","file":"AIML/gradient descent.md","subpath":"#gradient descent for simple linear regression","id":"1e7b3cbc8e894d7a","x":5807,"y":280,"width":453,"height":480},
		{"type":"text","text":"### Decision tree","id":"8c68f3ef08c49450","x":4743,"y":2856,"width":195,"height":50},
		{"type":"file","file":"AIML/Multiple Linear Regression.md","subpath":"#gradient descent for multiple linear regression","id":"2e6adbb74503a107","x":6034,"y":-591,"width":623,"height":571},
		{"type":"file","file":"screenshots/Pasted image 20230309194428.png","id":"b46217417c022682","x":-3159,"y":1033,"width":675,"height":171}
	],
	"edges":[
		{"id":"56aa0a9bbebb6b90","fromNode":"72ebb3e3d6b2058b","fromSide":"left","toNode":"a9a6a839bcf4162b","toSide":"right","label":"unscaled"},
		{"id":"e736129c5c3acf8e","fromNode":"45f22145efd1381c","fromSide":"left","toNode":"b46217417c022682","toSide":"right","label":"scaled"},
		{"id":"a4f7f93a665858dc","fromNode":"72ebb3e3d6b2058b","fromSide":"left","toNode":"45f22145efd1381c","toSide":"right"},
		{"id":"d88391f920b4489a","fromNode":"45f22145efd1381c","fromSide":"left","toNode":"e5b14976b0d7ed55","toSide":"right","label":"implementation"},
		{"id":"62b3e2f353a425e1","fromNode":"e5b14976b0d7ed55","fromSide":"left","toNode":"9209659129726005","toSide":"right","label":"output"},
		{"id":"db0906aee1e87df5","fromNode":"59a338dfada827d5","fromSide":"right","toNode":"e13c667c9d042035","toSide":"top","label":"sigmoid function and \ncorresponding \ndecision boundary"},
		{"id":"d5aa60c887046c76","fromNode":"59a338dfada827d5","fromSide":"right","toNode":"c3f11a7d714f6d35","toSide":"left"},
		{"id":"1919faffa0d2aa2a","fromNode":"c3f11a7d714f6d35","fromSide":"bottom","toNode":"15d224b9fb475c34","toSide":"top"},
		{"id":"9e05fa3d750d62b2","fromNode":"27c97ef539264f37","fromSide":"left","toNode":"bd9136c8538f1282","toSide":"right"},
		{"id":"3084920a3ec8b7e6","fromNode":"bd9136c8538f1282","fromSide":"left","toNode":"1c8696753c59742d","toSide":"right"},
		{"id":"be67e8873eb70bc8","fromNode":"1c8696753c59742d","fromSide":"left","toNode":"993226b02f894ec9","toSide":"right"},
		{"id":"50964514cb40f619","fromNode":"993226b02f894ec9","fromSide":"left","toNode":"0c9f4e0779b8cbf9","toSide":"right"},
		{"id":"0922cf0356caee7e","fromNode":"22e1a2f895acf05e","fromSide":"right","toNode":"1c4143ed866ab973","toSide":"left"},
		{"id":"2f6a124f510520a8","fromNode":"1c4143ed866ab973","fromSide":"right","toNode":"973ee40c93fc1cd9","toSide":"left","label":"problem type 1"},
		{"id":"1e7cd4b0285fd599","fromNode":"973ee40c93fc1cd9","fromSide":"right","toNode":"6c63638f6b152700","toSide":"left"},
		{"id":"5722f62e6df8eafe","fromNode":"6c63638f6b152700","fromSide":"bottom","toNode":"808b1d56f2480d41","toSide":"top","label":"one variable"},
		{"id":"6f05a5964413094a","fromNode":"808b1d56f2480d41","fromSide":"right","toNode":"dd2b17452f547aea","toSide":"left","label":"driven by"},
		{"id":"b29936b2fff4e52b","fromNode":"dd2b17452f547aea","fromSide":"right","toNode":"ea13afc362a21b4d","toSide":"left","label":"works with"},
		{"id":"092675780ad209d2","fromNode":"6c63638f6b152700","fromSide":"top","toNode":"410ec58058cb7e41","toSide":"bottom","label":"multi-variable"},
		{"id":"b8c096b68186e826","fromNode":"fa6f62713f039a5e","fromSide":"bottom","toNode":"410ec58058cb7e41","toSide":"top","label":"used in"},
		{"id":"8620cf993deaa13b","fromNode":"1c4143ed866ab973","fromSide":"right","toNode":"dc30a3ff2502f2be","toSide":"left","label":"problem type 2"},
		{"id":"5589563c5785bb99","fromNode":"dc30a3ff2502f2be","fromSide":"right","toNode":"09152c6f9e14d174","toSide":"left"},
		{"id":"2e3d23f4e7a24f2b","fromNode":"09152c6f9e14d174","fromSide":"right","toNode":"59a338dfada827d5","toSide":"left"},
		{"id":"9d879a0b54c58db9","fromNode":"410ec58058cb7e41","fromSide":"right","toNode":"2e6adbb74503a107","toSide":"left"},
		{"id":"e7e7b4ba77a255af","fromNode":"1e7b3cbc8e894d7a","fromSide":"left","toNode":"808b1d56f2480d41","toSide":"top"},
		{"id":"baaf12f9fa69e707","fromNode":"973ee40c93fc1cd9","fromSide":"top","toNode":"476a43ca8fb26d53","toSide":"bottom"},
		{"id":"307ff38492f4ea4d","fromNode":"c3f11a7d714f6d35","fromSide":"top","toNode":"dd2b17452f547aea","toSide":"right"},
		{"id":"16e9656709ae46f6","fromNode":"22e1a2f895acf05e","fromSide":"left","toNode":"27c97ef539264f37","toSide":"bottom"},
		{"id":"d876e15bc6bbe018","fromNode":"27c97ef539264f37","fromSide":"left","toNode":"72ebb3e3d6b2058b","toSide":"right"},
		{"id":"64473180b52d9c9e","fromNode":"27c97ef539264f37","fromSide":"left","toNode":"6c20ba169c6e4188","toSide":"top"},
		{"id":"23bd06667d1e8ce0","fromNode":"22e1a2f895acf05e","fromSide":"bottom","toNode":"b9cc88785fc0fc15","toSide":"top"},
		{"id":"aa91e6fecc384336","fromNode":"22e1a2f895acf05e","fromSide":"left","toNode":"ddf0d106d44106bb","toSide":"top"},
		{"id":"60b04e94530b997e","fromNode":"ea13afc362a21b4d","fromSide":"left","toNode":"1e7b3cbc8e894d7a","toSide":"right"},
		{"id":"d9118009dcb00dd3","fromNode":"ddf0d106d44106bb","fromSide":"left","toNode":"1802be618a31ebda","toSide":"right"},
		{"id":"c50617419069b08a","fromNode":"1802be618a31ebda","fromSide":"bottom","toNode":"0f8e2298c66887e5","toSide":"top","label":"solution"},
		{"id":"2ed0d76f7251d9c5","fromNode":"0f8e2298c66887e5","fromSide":"right","toNode":"e020b15b735b7492","toSide":"left"},
		{"id":"84db60aa39723562","fromNode":"e020b15b735b7492","fromSide":"right","toNode":"f83752f520ee7fb2","toSide":"left","label":"problem"},
		{"id":"d8310e04f19138a6","fromNode":"f83752f520ee7fb2","fromSide":"bottom","toNode":"23f4e8e6eab3ade5","toSide":"top","label":"solution"},
		{"id":"a54855d4899cccc7","fromNode":"0b587edb2dad7652","fromSide":"bottom","toNode":"f7beeb7d7c5ca525","toSide":"top"},
		{"id":"12c59e74165d6fc1","fromNode":"1a9028364ece0f59","fromSide":"right","toNode":"4bdeacbd00e16b59","toSide":"left"},
		{"id":"7047a5654435fa29","fromNode":"f25addc893353a68","fromSide":"right","toNode":"1a9028364ece0f59","toSide":"left"},
		{"id":"376d49cfbe11fad6","fromNode":"fb71847f75f136be","fromSide":"right","toNode":"4bdeacbd00e16b59","toSide":"left"},
		{"id":"953b21aea54a575e","fromNode":"f25addc893353a68","fromSide":"right","toNode":"fb71847f75f136be","toSide":"left"},
		{"id":"0719553140ede60b","fromNode":"ca34c19bf1dd01c5","fromSide":"right","toNode":"4bdeacbd00e16b59","toSide":"left"},
		{"id":"e4edccef8295604b","fromNode":"2581eecef4ff5ef6","fromSide":"right","toNode":"ca34c19bf1dd01c5","toSide":"left"},
		{"id":"9b4e321436c33927","fromNode":"2581eecef4ff5ef6","fromSide":"right","toNode":"feaf902c3a35a109","toSide":"left"},
		{"id":"f8fc7dab80549360","fromNode":"f25addc893353a68","fromSide":"right","toNode":"feaf902c3a35a109","toSide":"left"},
		{"id":"9eadce3b729fa63d","fromNode":"f25addc893353a68","fromSide":"right","toNode":"8c68f3ef08c49450","toSide":"left"},
		{"id":"cde994072e7c89a6","fromNode":"2581eecef4ff5ef6","fromSide":"right","toNode":"8c68f3ef08c49450","toSide":"left"},
		{"id":"97c20cb0b815bda8","fromNode":"2581eecef4ff5ef6","fromSide":"bottom","toNode":"96cd2ff4029651ac","toSide":"left"},
		{"id":"4b9a4c982c52529a","fromNode":"2581eecef4ff5ef6","fromSide":"bottom","toNode":"a06f23b7a7ca66ce","toSide":"left"},
		{"id":"65f79cfd03aa4691","fromNode":"7f9c8775eb9fae08","fromSide":"right","toNode":"f25addc893353a68","toSide":"left"},
		{"id":"2836921807486073","fromNode":"7f9c8775eb9fae08","fromSide":"right","toNode":"2581eecef4ff5ef6","toSide":"left"},
		{"id":"afce03717d6f6b3f","fromNode":"7f9c8775eb9fae08","fromSide":"bottom","toNode":"56669d8ee483f171","toSide":"left"},
		{"id":"60a3bc5fb2cf7f47","fromNode":"0b587edb2dad7652","fromSide":"right","toNode":"7f9c8775eb9fae08","toSide":"left"},
		{"id":"23b96b11fe71ad39","fromNode":"0b587edb2dad7652","fromSide":"top","toNode":"9c3f695211aaaf8b","toSide":"bottom"},
		{"id":"968185cb1af4bb31","fromNode":"2581eecef4ff5ef6","fromSide":"bottom","toNode":"b633c59ee9509b72","toSide":"left"},
		{"id":"5278a9c4eb2d822d","fromNode":"0b587edb2dad7652","fromSide":"left","toNode":"c4fb5b8454fcf909","toSide":"right"},
		{"id":"aa8be404a6e474a4","fromNode":"0b587edb2dad7652","fromSide":"left","toNode":"0995bd2cea90de16","toSide":"right"},
		{"id":"fbef484ebc48c0e4","fromNode":"993226b02f894ec9","fromSide":"left","toNode":"76393e5343e2ff1c","toSide":"right"},
		{"id":"69a04c1f5568fa00","fromNode":"23f4e8e6eab3ade5","fromSide":"left","toNode":"6e993623904444e7","toSide":"right"},
		{"id":"955f05177bb6896b","fromNode":"6e993623904444e7","fromSide":"left","toNode":"8a56595189133be2","toSide":"right"},
		{"id":"c463ae9f771884bf","fromNode":"23f4e8e6eab3ade5","fromSide":"right","toNode":"5e04729becceff97","toSide":"left"},
		{"id":"d7343ac7999f5718","fromNode":"5e04729becceff97","fromSide":"bottom","toNode":"21869d20c11f1d42","toSide":"top"},
		{"id":"b290e7c9bec67522","fromNode":"6e993623904444e7","fromSide":"bottom","toNode":"7cbb0616917170e2","toSide":"top","label":"when \nhigh bias"},
		{"id":"ebb15ab321c1357c","fromNode":"6e993623904444e7","fromSide":"bottom","toNode":"0c6ad714dce8ce60","toSide":"top","label":"when\nhigh variance"},
		{"id":"614f1fb78682c1d5","fromNode":"84b3c58bd8d74902","fromSide":"top","toNode":"f42570a0e1362ce1","toSide":"bottom","label":"no"},
		{"id":"03bffaf60d974260","fromNode":"c443da954021fc1a","fromSide":"bottom","toNode":"8e3b804d866ff08a","toSide":"top","label":"no"},
		{"id":"1207d3bef087239e","fromNode":"c443da954021fc1a","fromSide":"right","toNode":"84b3c58bd8d74902","toSide":"left","label":"yes"},
		{"id":"813650811daaaf7d","fromNode":"84b3c58bd8d74902","fromSide":"right","toNode":"95eeb62441dc1a15","toSide":"left","label":"yes"},
		{"id":"743cfb7074770d69","fromNode":"8e3b804d866ff08a","fromSide":"left","toNode":"c443da954021fc1a","toSide":"left"},
		{"id":"9e7c3698f1e818db","fromNode":"f42570a0e1362ce1","fromSide":"left","toNode":"c443da954021fc1a","toSide":"top"},
		{"id":"d96899e1b33c6337","fromNode":"6e993623904444e7","fromSide":"left","toNode":"ac54b113c1317a39","toSide":"right"},
		{"id":"d0ce7c32615d87b9","fromNode":"22e1a2f895acf05e","fromSide":"top","toNode":"dc7c01494986628a","toSide":"bottom"},
		{"id":"d183a7eaa21dd8f2","fromNode":"dc7c01494986628a","fromSide":"top","toNode":"8d6165954ab862a8","toSide":"bottom"}
	]
}