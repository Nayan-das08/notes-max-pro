{
	"nodes":[
		{"type":"group","id":"8e01171f9dbc6704","x":-1760,"y":-2970,"width":2048,"height":1585,"label":"calculation of activation of neurons"},
		{"type":"group","id":"59bab04eb8a6fa09","x":2837,"y":742,"width":2663,"height":864,"label":"Blueprint"},
		{"type":"group","id":"aa473df9ccf34c23","x":2120,"y":-3200,"width":520,"height":460,"label":"example"},
		{"type":"file","file":"AIML/Machine Learning.md","id":"22e1a2f895acf05e","x":1749,"y":673,"width":524,"height":400},
		{"type":"text","text":"## inputs to a neuron\n\n- They are sent as a vector $\\vec {\\mathbf{x}}$ consisting of $x_i$ elements","id":"270a3b62cbc269bd","x":-60,"y":-2155,"width":328,"height":141},
		{"type":"text","text":"## weights to inputs\n\n- these are associated to each input to a neuron\n- represents the weight/importance attached to the input\n- represented as vector $\\vec{\\mathbf w}$ consisting of weight $w_i$ corresponding to input $x_i$.","id":"6d223a6ecccfde5a","x":-60,"y":-1975,"width":328,"height":320},
		{"type":"text","text":"## bias\n- added to each neuron to shift the product of weights and inputs linearly","id":"b8d255723be2141b","x":-60,"y":-1625,"width":328,"height":200},
		{"type":"text","text":"$\\vec{\\mathbf{x}}$","id":"9a5606177e06e161","x":2160,"y":-2828,"width":57,"height":50},
		{"type":"text","text":"## Commonly used Activation Functions\n- **Linear Function** \n\t- $$f(x) = x$$\n\t- same value as input\n- **Sigmoid Function** \n\t- $$f(x) = \\frac{1}{1+e^{-x}}$$\n\t- map the value between 0 and 1\n- **Tanh Function** \n\t- $$f(x) = \\frac{e^x-e^{-x}}{e^x+e^{-x}}$$\n\t- map the value between -1 and 1\n\n- **Softmax Function** \n\t- $$f(\\vec{\\mathbf{z}})_i = \\frac{e^{z_i}}{\\sum_{j=1}^{K}e^{z_j}}$$\n\t- apply logistic function on multiple dimensions\n- **ReLU Function** \n\t- $$f(x) = max(0,x) = \\begin{cases} x & x \\ge 0 \\\\ 0 & x < 0 \\end{cases}$$\n\t- faster, vanishing gradient (in above two)\n- **Leaky ReLU Function** \n\t- $$f(x) = max(0,x) = \\begin{cases} x & x \\ge 0 \\\\ \\alpha x & x < 0 \\end{cases}$$\n\t- solves dying gradient","id":"746b2fdd8175fa3a","x":2228,"y":-2300,"width":492,"height":970},
		{"type":"text","text":"## sigmoid vs. softmax\n\nhttps://web.stanford.edu/~nanbhas/blog/sigmoid-softmax/","id":"b02b092eb94b90de","x":2840,"y":-2170,"width":256,"height":246},
		{"type":"text","text":"# Activation Functions\n\nthe are the functions which give the *required transformation* on the $z_j = w_{1j}x_1 + w_{2j}x_2$, i.e.\n$$a = f(z)$$\nThe linear transformation obtained from weighted sum can only model a linear relationship between inputs and outputs. Real world problems are much more complex and are non-linear in nature. \n\nActivation functions add the non-linearity required to the decision making process (forward propagation). It helps understand / represent complex patterns and relationships in data.\n\n","id":"baf6c1e4052843f7","x":1400,"y":-2070,"width":612,"height":420},
		{"type":"text","text":"# Artificial Neuron\n\n- It is the building block of a Neural Network\n- It receives inputs from multiple neurons and gives out a single output\n- that output is then sent to multiple neurons \n\nThe output of a neuron is called *activation* and is given by\n$$a = f\\left(\\sum_{i=1}^{n}w_ix_i+b\\right)=f(\\vec{\\mathbf{w}}\\cdot \\vec{\\mathbf{b}}+c)=f(z)$$\nfor activation function $f(\\cdot)$.\n\n$w_i$ represents the weight associated to input $x_i$.","id":"dabcf3923fc55e49","x":520,"y":-1970,"width":540,"height":480},
		{"type":"text","text":"## calculation of activation *$a$* (for a system of nodes)\n\nevaluate the activation function of weighted sum vector $\\vec{\\mathbf{z}}$ after adding a bias $b$\n$$\\vec{\\mathbf{a}} = f(\\vec{\\mathbf{z}}+b)$$\n$$\n\\vec{\\mathbf{a}} = \n\\begin{bmatrix}\na_1 \\\\\na_2 \\\\\na_3 \\\\\n\\end{bmatrix} =\n\\begin{bmatrix}\nf(z_1 + b) \\\\\nf(z_2 + b) \\\\\nf(z_3 + b) \\\\\n\\end{bmatrix} =f\n\\left(\n\\begin{bmatrix}\nz_1 \\\\\nz_2 \\\\\nz_3 \\\\\n\\end{bmatrix}\n+ b\n\\right)\n$$","id":"3bdb66d11398bb34","x":-758,"y":-2456,"width":576,"height":341},
		{"type":"text","text":"## calculation of weighted sum *$z$* (for system of node)\n\nfor a system of\n- 2 neurons in *input layer*\n- 3 neurons in *current layer*\n\nwe have\n\n$$\n\\mathbf{W} = \\begin{bmatrix}\nw_{11} & w_{12} & w_{13} \\\\\nw_{21} & w_{22} & w_{23} \\\\\n\\end{bmatrix}\n\\hspace{1cm}\n\\vec {\\mathbf{x}} = \\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\n\\end{bmatrix}\n\\hspace{1cm}\n\\vec {\\mathbf{z}} = \\begin{bmatrix}\nz_1 \\\\\nz_2 \\\\\nz_3 \\\\\n\\end{bmatrix}\n$$\n$i$ = neuron$_i$ in current layer = $z_i$ ($z$ represents weighted sum)\n$j$ = input$_j$ = $x_j$ \n\nwhere\n- in $\\mathbf{W}$\n\t- $i^{th}$ row, i.e. $[w_{i1}, w_{i2}, w_{i3}]$ represents the weights to the input $x_i$ \n\t- $j^{th}$ column, i.e. $[w_{1j}, w_{2j}]$ represents the 2 weights that are going in $z_i$ \n- in $\\vec{\\mathbf{x}}$\n\t- $i^{th}$ row, i.e. $x_i$ is the $i^{th}$ input\n- in $\\vec{\\mathbf{z}}$\n\t- $i^{th}$ row, i.e. $z_i$ is the weighted sum of inputs $i^{th}$ neuron in current layer\n\nthe weighted sum is given as:\n$$\n\\vec{\\mathbf{z}}=\n\\begin{bmatrix}\nz_1 \\\\\nz_2 \\\\\nz_3 \\\\\n\\end{bmatrix}=\n\\begin{bmatrix}\nw_{11}x_1 + w_{21}x_2 \\\\\nw_{12}x_1 + w_{22}x_2 \\\\ \nw_{13}x_1 + w_{23}x_2 \\\\\n\\end{bmatrix}\n$$\nthus, $\\vec{\\mathbf{z}}$ is obtained by dot product of $\\mathbf{W}$ and $\\vec{\\mathbf{x}}$ or matrix multiplication of $\\mathbf{W}^T$ and $\\vec{\\mathbf{x}}$, i.e.\n$$\\vec{\\mathbf{z}} = \\mathbf{W} \\cdot \\vec{\\mathbf{x}} = \\mathbf{W}^T \\vec{\\mathbf{x}}$$\n$$\n\\vec {\\mathbf{z}} = \\begin{bmatrix}\nz_1 \\\\\nz_2 \\\\\nz_3 \\\\\n\\end{bmatrix}=\n\\mathbf{W}^T \\vec {\\mathbf{x}} = \\begin{bmatrix}\nw_{11} & w_{21} \\\\\nw_{12} & w_{22} \\\\\nw_{13} & w_{23} \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\n\\end{bmatrix}=\n\\begin{bmatrix}\nw_{11}x_1 + w_{21}x_2 \\\\\nw_{12}x_1 + w_{22}x_2 \\\\ \nw_{13}x_1 + w_{23}x_2 \\\\\n\\end{bmatrix}=\n\\vec{\\mathbf{z}}=\\begin{bmatrix}\nz_1 \\\\\nz_2 \\\\\nz_3 \\\\\n\\end{bmatrix}\n$$","id":"868b127a1d026bcb","x":-1740,"y":-2430,"width":700,"height":1025},
		{"type":"text","text":"The weight matrix can be denoted as vector of column vectors\n$$\n\\mathbf{W} = \\begin{bmatrix}\nw_{11} & w_{12} & w_{13} \\\\\nw_{21} & w_{22} & w_{23} \\\\\n\\end{bmatrix} =\n[\\vec{\\mathbf{w}}_1, \\vec{\\mathbf{w}}_2, \\vec{\\mathbf{w}}_3]\n$$\nwhere\n$$\n\\vec{\\mathbf{w}}_j = \n\\begin{bmatrix}\nw_{1j} \\\\\nw_{2j} \\\\\n\\end{bmatrix}\n$$\nis the weight associated to $z_j$, i.e.\n$$z_j = w_{1j}x_1 + w_{2j}x_2$$ ","id":"12913731189cd991","x":-1498,"y":-2894,"width":498,"height":364},
		{"type":"text","text":"Since\n$$\n\\vec{\\mathbf{w}}_j=\n\\begin{bmatrix}\n\tw_{j1} \\\\\n\tw_{j2} \\\\\n\\end{bmatrix}, \\hspace{1cm}\n\\vec{\\mathbf{x}}=\n\\begin{bmatrix}\n\tx_1 \\\\\n\tx_2 \\\\\n\\end{bmatrix}\n$$\nwe can have\n$$\nz_j = w_{1j}x_1 + w_{2j}x_2 = \\vec{\\mathbf{w}}_j \\cdot \\vec{\\mathbf{x}}\n$$\nand\n$$\n\\vec{\\mathbf{z}} = \n\\begin{bmatrix}\nz_1 \\\\\nz_2 \\\\\nz_3 \\\\\n\\end{bmatrix} = \n\\begin{bmatrix}\nw_{11}x_1 + w_{21}x_2 \\\\\nw_{12}x_1 + w_{22}x_2 \\\\\nw_{13}x_1 + w_{23}x_2 \\\\\n\\end{bmatrix} = \n\\begin{bmatrix}\n\\vec{\\mathbf{w}}_1 \\cdot \\vec{\\mathbf{x}} \\\\\n\\vec{\\mathbf{w}}_2 \\cdot \\vec{\\mathbf{x}} \\\\\n\\vec{\\mathbf{w}}_3 \\cdot \\vec{\\mathbf{x}} \\\\\n\\end{bmatrix}\n$$\n\n\n","id":"ccc7ecbc63c10dd3","x":-882,"y":-2930,"width":478,"height":400},
		{"type":"text","text":"$a^{[1]}$","id":"d2648fff1dac9aa1","x":2260,"y":-2828,"width":80,"height":50},
		{"type":"text","text":"$a^{[2]}$","id":"65a0fca551fcd410","x":2400,"y":-2828,"width":80,"height":50},
		{"type":"text","text":"$a^{[3]}$","id":"62474b3fb62a770c","x":2540,"y":-2828,"width":80,"height":50},
		{"type":"text","text":"# to do\n\n- [ ] computation graph\n- [ ] Backpropagation","id":"e4f30a0d7340e9af","x":2840,"y":-2828,"width":373,"height":427},
		{"type":"link","url":"https://www.youtube.com/watch?v=QDX-1M5Nj7s&list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI&pp=iAQB","id":"f543016a9f878d6e","x":-3039,"y":-2665,"width":1050,"height":690},
		{"type":"text","text":"## calculation of activation *$a$* (for one node)\n\nfor a neuron getting $n$ inputs (from previous layer),\n- $\\vec {\\mathbf w}$: weights associated to inputs\n- $\\vec {\\mathbf x}$: input vector\n\n$$\n\\vec {\\mathbf{w}} = \\begin{bmatrix}\nw_1 \\\\\nw_2 \\\\\n\\vdots \\\\\nw_n\n\\end{bmatrix}\n\\hspace{1cm}\n\\vec {\\mathbf{x}} = \\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\n\\vdots \\\\\nx_n\n\\end{bmatrix}\n$$\nthe activation $a$ is determined as:\n$$\n\\begin{align}\na &= f\\left(\\sum_{i=1}^{n}w_ix_i + b \\right) \\\\\na &= f\\left(\\vec{\\mathbf{w}} \\cdot \\vec{\\mathbf{x}} + b \\right)\n\\end{align}\n$$\nwhere, $\\vec{\\mathbf{w}} \\cdot \\vec{\\mathbf{x}}$ acts as the weighted sum of inputs\n$$\n\\begin{align}\n\\vec{\\mathbf{w}} \\cdot \\vec{\\mathbf{x}} &= [w_1x_1+w_2x_2+ \\cdots+w_nx_n] \\\\\n&= \\sum_{i=1}^{n}w_ix_i\n\\end{align}\n$$\n","id":"d1c08531261a06b7","x":-882,"y":-2045,"width":700,"height":630},
		{"type":"file","file":"screenshots/Pasted image 20230604025315.png","id":"8609f311ff16571c","x":2140,"y":-3160,"width":469,"height":320},
		{"type":"text","text":"## calculation of weighted sum *$z$* (for a system of layers)\n\nimagine a neural network with\n- *2 neurons* in input layer (layer 0)\n- *4 neurons* in layer 1\n- *3 neurons* in layer 2\n- *1 neuron* in output layer (layer 3)\n\nThe output of layer 1, 2 and 3 are $a^{[1]}$, $a^{[2]}$ and $a^{[3]}$, respectively","id":"31b104c1ba6c677a","x":1224,"y":-3145,"width":560,"height":342},
		{"type":"text","text":"## calculation of *$a^{[1]}$*\n\n$$\n\\vec{\\mathbf{a}}^{[1]} = f(\\mathbf{W}^{[1]} \\cdot \\vec{\\mathbf{x}} + \\vec{\\mathbf{b}}^{[1]})\n$$\n\n$$\n\\vec{\\mathbf{a}}^{[1]} = \n\\begin{bmatrix}\na_1^{[1]} \\\\\na_2^{[1]} \\\\\na_3^{[1]} \\\\\na_4^{[1]} \\\\\n\\end{bmatrix} = \n\\begin{bmatrix}\nf(\\vec{\\mathbf{w}}^{[1]}_1 \\cdot \\vec{\\mathbf{x}} + b_1^{[1]}) \\\\\nf(\\vec{\\mathbf{w}}^{[1]}_2 \\cdot \\vec{\\mathbf{x}} + b_2^{[1]}) \\\\\nf(\\vec{\\mathbf{w}}^{[1]}_3 \\cdot \\vec{\\mathbf{x}} + b_3^{[1]}) \\\\\nf(\\vec{\\mathbf{w}}^{[1]}_4 \\cdot \\vec{\\mathbf{x}} + b_4^{[1]}) \\\\\n\\end{bmatrix}\n$$","id":"3552bb46c3fd2fef","x":1224,"y":-2543,"width":393,"height":286},
		{"type":"text","text":"## calculation of *$a^{[2]}$*\n\n$$\n\\vec{\\mathbf{a}}^{[2]} = f(\\mathbf{W}^{[2]} \\cdot \\vec{\\mathbf{a}}^{[1]} + \\vec{\\mathbf{b}}^{[2]})\n$$\n\n$$\n\\vec{\\mathbf{a}}^{[2]} = \n\\begin{bmatrix}\na_1^{[2]} \\\\\na_2^{[2]} \\\\\na_3^{[2]} \\\\\n\\end{bmatrix} = \n\\begin{bmatrix}\nf(\\vec{\\mathbf{w}}^{[2]}_1 \\cdot \\vec{\\mathbf{a}}^{[1]} + b_1^{[2]}) \\\\\nf(\\vec{\\mathbf{w}}^{[2]}_2 \\cdot \\vec{\\mathbf{a}}^{[1]} + b_2^{[2]}) \\\\\nf(\\vec{\\mathbf{w}}^{[2]}_3 \\cdot \\vec{\\mathbf{a}}^{[1]} + b_3^{[2]}) \\\\\n\\end{bmatrix}\n$$","id":"cc98780ad53930d4","x":1660,"y":-2543,"width":393,"height":243},
		{"type":"text","text":"## calculation of *$a^{[3]}$*\n\n$$\n\\vec{\\mathbf{a}}^{[3]} = f(\\mathbf{W}^{[2]} \\cdot \\vec{\\mathbf{a}}^{[2]} + \\vec{\\mathbf{b}}^{[3]})\n$$\n\n$$\n\\vec{\\mathbf{a}}^{[3]} = \n\\begin{bmatrix}\na_1^{[3]} \\\\\n\\end{bmatrix} = \n\\begin{bmatrix}\nf(\\vec{\\mathbf{w}}^{[3]}_1 \\cdot \\vec{\\mathbf{a}}^{[2]} + b_1^{[3]}) \\\\\n\\end{bmatrix}\n$$","id":"56d15d44c657d39b","x":2104,"y":-2543,"width":393,"height":180},
		{"type":"text","text":"# Neural Network Layers\n\nThe neural networks typically use a layer of such neurons between the input layer and the output layer. These are called the ***Hidden Layers***.\n\nThe output from one layer becomes the input to the next layer.","id":"e733ceb8d63fa346","x":520,"y":-2693,"width":442,"height":263},
		{"type":"file","file":"screenshots/tf_layers.pdf","id":"a2e1d233f8e29c98","x":-404,"y":-3965,"width":900,"height":820},
		{"type":"text","text":"```python\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\n\n# create a toy dataset\nX_train = np.array([[1., 2.],\n                    [3., 4.],\n                    [5., 6.]])\n\n# create a MinMaxScaler object and fit the training data\nscaler = MinMaxScaler()\nscaler.fit(X_train)\n\n# transform the training data using the learned parameters\nX_train_scaled = scaler.transform(X_train)\n\n# print the scaled data\nprint(X_train_scaled)\n\n```","id":"e5b14976b0d7ed55","x":-582,"y":2719,"width":617,"height":501},
		{"type":"text","text":"```\n[[0.   0. ]\n [0.5  0.5]\n [1.   1. ]]\n```","id":"9209659129726005","x":-967,"y":3023,"width":196,"height":144},
		{"type":"file","file":"screenshots/Pasted image 20230309194213.png","id":"a9a6a839bcf4162b","x":-249,"y":1987,"width":399,"height":111},
		{"type":"file","file":"screenshots/Pasted image 20230309194428.png","id":"b46217417c022682","x":-1327,"y":2282,"width":1362,"height":345},
		{"type":"text","text":"# Feature Scaling\n\n## Why Feature Scaling is needed?\nWhen a feature of a model has a range of values that is very large (magnitude wise), the learning algorithms sets the corresponding weight very small, and vice versa.\n\nBecause of this, the 2D-contour the cost function on two features (say one which has a large range of values and another with short range) is *elliptical*.\n\nSuppose, \n- $x_1$ is the feature with large range, thus $w_1$ would be small.\n- $x_2$ is the feature with short range, thus $w_2$ would be large.\n\nTherefore,\n- A small change in $w_1$ will cause a huge change in resulting product $w_1x_1$. Thus contour will be narrow across this axis\n- A small change in $w_2$ will cause a small change in resulting product $w_2x_2$. Thus contour will be broad across this axis.\n\nSince, the cost function curve is skewed, when the model is tested, the gradient descent algo will make the weight values bounce/oscillate the narrow axis","id":"72ebb3e3d6b2058b","x":443,"y":1387,"width":670,"height":675},
		{"type":"text","text":"## How to perform feature scaling\nIf the values of both $x_1$ and $x_2$ are in the same/comparable range, the contour would be circular.\n\n1. **Min-Max scaling / Normalization**\n\t- maps between 0 and 1\n\t- sensitive to outliers\n\t- $$x_{new}=\\frac{x_i-min(x_i)}{max(x_i)-min(x_i)}$$\n2. Standardization / Z-score scaling\n\t- resulting data will have mean=0, std. dev.=1\n\t- less sensitive to outliers\n\t- $$x_{new}=\\frac{x_i-x_{mean}}{\\sigma}$$\n3. Robust scaling\n4. Logarithmic scaling\n5. Power scaling\n6. Unit vector scaling","id":"45f22145efd1381c","x":358,"y":2354,"width":610,"height":669},
		{"type":"text","text":"<h1 style=\"text-align:center; font-size:80\">Improving Models</h1>\n\n","id":"27c97ef539264f37","x":268,"y":483,"width":768,"height":190},
		{"type":"text","text":"# Deep Learning\n\nA *branch* (or rather an implementation of) of *Artificial Intelligence* which is concerned with mimicking the working of the Human Brain Cell **Neuron**\n\nDeep learning concerns with **Artificial Neural Networks** (and its various types), made of **Artificial Neurons**. \n\n## advantages\n- automatic feature selection and engineering from a vector of inputs","id":"f48d63a2427d788c","x":1193,"y":-1200,"width":513,"height":390},
		{"type":"text","text":"# intuition for Neural Networks\n\nwhen creating a *Machine Learning* prediction model, suppose for binary classification with *logistic regression*, we use the model \n$$f(x)_{W,b} = \\frac{1}{1+e^{wx+b}}$$\nthis can ","id":"56fd3a04045d87aa","x":268,"y":-1200,"width":631,"height":447},
		{"type":"text","text":"<h1 style=\"text-align:center; font-size:80\">Machine Learning</h1>\n\n","id":"07d5d14ae25284e8","x":1628,"y":-246,"width":768,"height":180},
		{"type":"text","text":"## Underfitting\n- when the model *doesn't fit the data* very well $\\implies$ the model is *underfitting the data* $\\implies$ the model has *high bias*\n- eg: the given data suggests that the target is dependent on input quadratically, but the applied model is only linear\n- **bias** here refers to a preconceived notion or assumption that the data is like something else. The model is not able to capture the pattern in the data.\n- solution: **more complex model**\n\n## Best Fit\n- the model fits the data *pretty well*\n- it does not have to fit all the samples of the training set, but rather fits the data just good enough.\n- this ensures that the model will perform/predict well even on new and unseen testing samples $\\implies$ *generalization*\n\n## Overfitting\n- when the model tries to fit over all the samples of the training set, even the noise and outliers $\\implies$ the model *overfits the data* $\\implies$ the model has *high variance*.\n- this fits the data *so extremely well* that the error or cost on training samples are completely zero\n- this ends up causing problems in prediction of new test samples and would fail to generalize over new samples\n- solution: **get more training data**, **regularization**, **dropout (in DL)**, etc.","id":"bd9136c8538f1282","x":-790,"y":166,"width":677,"height":880},
		{"type":"text","text":"# Solving Overfitting\n\nSome methods to address the problem of overfitting are:\n1. **increase number of training samples**\n\t- this prevents the model from being affected severely due to the noise\n\t- abundance of data is not always available\n2. **feature selection** \n\t- if we have a lot of features and insufficient data, we may encounter overfitting\n\t- if we select only a subset of relevant features to create a model we can get best fit case\n\t- feature selection leads to not so important features not being acknowledged by the model\n3. **Regularization**\n\t- in most overfit models, the weights are often very large. \n\t- regularization is a method of gently reducing the impact of some of the features without eliminating them right away ","id":"1c8696753c59742d","x":-1667,"y":252,"width":681,"height":588},
		{"type":"text","text":"# Regularization\n\n- The method of *reducing the impact of parameters* on features where the parameters have huge values and are causing **overfitting**. \n- smaller values of parameters $\\implies$ simpler model $\\implies$ less prone to overfitting\n\n## performing regularization\n- if we have many features, we may not know which features are to be penalized and reduced.\n- thus we penalize all the features\n- we penalize them by adding a new term to the cost function: a **regularization term**\n$$\\begin{gather}\nJ(W,b)=\\frac{1}{2m}\\sum_{i=1}^{m} (f_{W,b}(X)-y^{(i)})^2 + \\frac{\\lambda}{2m} \\sum_{j=1}^{n}w_j^2\\\\\\\\\n\\text{error term}=\\frac{1}{2m}\\sum_{i=1}^{m} (f_{W,b}(X)-y^{(i)})^2 \\\\\n\\text{regularization term}=\\frac{\\lambda}{2m} \\sum_{j=1}^{n}w_j^2\n\\end{gather}\n$$\n- we multiply by $\\frac{1}{2m}$ to have both the terms scaled \n- $\\lambda$ is a hyperparameter, just like $\\alpha$, the learning rate\n\t- keeping $\\lambda$ very big (say $10^{10}$) places very heavy weight on the regularization term of cost function\n\t- the model ends up minimizing all the weights very close to zero\n\t- results in *underfitting*\n\t- model needs to be checked for multiple orders of values for $\\lambda$ to determine the appropriate range","id":"993226b02f894ec9","x":-1834,"y":1046,"width":700,"height":654},
		{"type":"text","text":"# Regularization in gradient descent\n\n## how the weights are minimized\n$$\\begin{align}\nw_j&=w_j-\\alpha\\left[\\nabla (\\text{error term}) + \\nabla (\\text{regularization term})\\right]\\\\\n&=w_j-\\alpha\\left[\\nabla (\\text{error term})+\\frac{\\lambda}{m}w_j \\right] \\\\\n&=w_j-\\alpha \\frac{\\lambda}{m}w_j-\\alpha \\nabla (\\text{error term}) \\\\\n&=w_j\\left(1-\\alpha\\frac{\\lambda}{m}\\right)-\\alpha \\nabla (\\text{error term})\n\\end{align}$$\n- here, the term $w_j\\left(1-\\alpha\\frac{\\lambda}{m}\\right)$ ensures that the $\\alpha \\nabla (\\text{error term})$ is not subtracted from $w_j$, but rather a fraction of $w_j$\n- suppose $\\lambda=1$, $m=50$ and $\\alpha=0.01$.\n- then $w_j\\left(1-\\alpha\\frac{\\lambda}{m}\\right)=(0.9998)w_j$","id":"0c9f4e0779b8cbf9","x":-2694,"y":1080,"width":580,"height":520},
		{"type":"text","text":"# Feature Engineering\n\nUsing intuition to design *new features* by transforming or combining original features\n\nSuppose for a model,\n$$f_{W,b}(X)=w_1x_1+w_2x_2+b$$\nwhere $x_1$ is length, $x_2$ is breadth and we need to predict the price of the item using the model\n\nAnother effective way to create the model: incorporate the *area* $x_3=x_1x_2$ as well. Then the model would be\n$$f_{W,b}(X)=w_1x_1+w_2x_2+w_3x_3+b$$\n\nThis makes it easier for the model to make accurate predictions","id":"6c20ba169c6e4188","x":-601,"y":1340,"width":637,"height":513},
		{"type":"text","text":"<h1 style=\"text-align:center; font-size:80\">Model Evaluation</h1>\n\n","id":"f51f992498b8187f","x":-27,"y":-426,"width":768,"height":180},
		{"type":"file","file":"AIML/Supervised Machine Learning.md","subpath":"#Classification","id":"36d6caffca6a8dbb","x":2981,"y":2078,"width":449,"height":374},
		{"type":"file","file":"AIML/Supervised Machine Learning.md","subpath":"#Regression","id":"973ee40c93fc1cd9","x":2197,"y":2339,"width":449,"height":374},
		{"type":"text","text":"![[Supervised Machine Learning#^376c22]]","id":"476a43ca8fb26d53","x":3038,"y":2983,"width":459,"height":178},
		{"type":"text","text":"![[Supervised Machine Learning#^5450a3]]","id":"0176eef9c6394291","x":1883,"y":3023,"width":387,"height":138},
		{"type":"text","text":"# different methods of classification\n\n- ### Logistic Regression\n- ### K-Nearest Neighbours\n- ### others\n","id":"496d02c68a5c4da9","x":3999,"y":2473,"width":400,"height":307},
		{"type":"file","file":"AIML/Logistic Regression.md","id":"59a338dfada827d5","x":4811,"y":2373,"width":679,"height":620},
		{"type":"file","file":"screenshots/Pasted image 20230312131823.png","id":"e13c667c9d042035","x":5965,"y":2253,"width":650,"height":430},
		{"type":"file","file":"AIML/gradient descent.md","id":"c0bdf6dcf747938d","x":3672,"y":3977,"width":527,"height":488},
		{"type":"text","text":"## checking convergence of gradient descent\n\nLet $\\epsilon$ be a small number (say $10^{-3}$ ).\n\nIf $J(W,b) \\le \\epsilon$ : convergence has been reached ","id":"9e980a31b13e06b3","x":3726,"y":4824,"width":419,"height":279},
		{"type":"file","file":"AIML/cost function.md","id":"54f6685e9a74b7e7","x":3951,"y":3072,"width":574,"height":400},
		{"type":"text","text":"![[gradient descent#types of gradient descent]]","id":"993f8ab72d7b3cfb","x":4451,"y":3878,"width":501,"height":377},
		{"type":"file","file":"AIML/Logistic Regression.md","subpath":"#cost function for logistic regression","id":"c3f11a7d714f6d35","x":5061,"y":3137,"width":665,"height":538},
		{"type":"file","file":"screenshots/Pasted image 20230312144702.png","id":"15d224b9fb475c34","x":6191,"y":3403,"width":759,"height":317},
		{"type":"text","text":"## Vectorization\nWhen handling a number of computations, like in Multiple Linear Regression where a number of multiplications is to be done; instead of using indexing (in for loops) to calculate the value of \n$$\\begin{align}\nf_{W,b}(X)=W\\cdot X + b \\\\\nf_{W,b}(X)= \\left( \\sum^{n}_{j=1} w_j x_j \\right) + b\n\\end{align}$$\nwe can use vectorization in *python code* to implement parallel computation of arithmetic operations for all $j$ simultaneosly.\n\nThis can be done using `numpy.dot(W,X)` function in Numpy.\n\n","id":"05af23aaea7a20f7","x":1113,"y":3867,"width":481,"height":490},
		{"type":"file","file":"AIML/Multiple Linear Regression.md","subpath":"#gradient descent for multiple linear regression","id":"8ba726aeedb83a04","x":1851,"y":4404,"width":623,"height":571},
		{"type":"file","file":"AIML/Multiple Linear Regression.md","id":"0cbc1ee4808852ff","x":1923,"y":3635,"width":694,"height":486},
		{"type":"file","file":"AIML/gradient descent.md","subpath":"#gradient descent for simple linear regression","id":"4c0c49f93c4ee8d6","x":2791,"y":4255,"width":605,"height":520},
		{"type":"file","file":"AIML/Simple Linear Regression.md","id":"35afa7953c9f73f1","x":2998,"y":3472,"width":539,"height":406},
		{"type":"file","file":"AIML/Supervised Machine Learning.md","id":"1c4143ed866ab973","x":1896,"y":1536,"width":449,"height":374},
		{"type":"text","text":"# Machine Learning","id":"44bdf185f87d831b","x":3357,"y":1073,"width":341,"height":195},
		{"type":"text","text":"## unsupervised","id":"5797afa29b160333","x":3406,"y":1417,"width":244,"height":169},
		{"type":"text","text":"## Supervised","id":"82e4fd2b7978ef6d","x":3826,"y":1118,"width":221,"height":105},
		{"type":"text","text":"## Deep Learning","id":"7304b3a20e29c219","x":3417,"y":762,"width":221,"height":105},
		{"type":"text","text":"## improving models\n### Feature Scaling, Feature Selection, Feature Engg, Overfitting, regularization","id":"455cad7a33c93b0c","x":2857,"y":929,"width":300,"height":211},
		{"type":"text","text":"## model evaluation\n### cross-validation, errors, bias-variance","id":"03e1a3385909d0d3","x":2881,"y":1237,"width":263,"height":157},
		{"type":"text","text":"## regression problem","id":"ad32b45e5becbf8f","x":4238,"y":871,"width":249,"height":122},
		{"type":"text","text":"## classification problem","id":"c8889ed551acd0fa","x":4238,"y":1044,"width":249,"height":122},
		{"type":"text","text":"## Other algo\n### XG-Boost, SVM, Decision Tree, Random Forest, Naiive Bayse","id":"4e08ff18ae6e0430","x":4084,"y":1316,"width":309,"height":169},
		{"type":"text","text":"## simple","id":"da2bb98cf2727930","x":4832,"y":822,"width":250,"height":60},
		{"type":"text","text":"## multiple","id":"3b69bad5825859ef","x":4832,"y":933,"width":250,"height":60},
		{"type":"text","text":"## logistic","id":"66b3548b53839827","x":4835,"y":1022,"width":250,"height":60},
		{"type":"text","text":"### grad descent and cost function","id":"252e5d265aba4062","x":5200,"y":852,"width":250,"height":97},
		{"type":"text","text":"### XG-Boost","id":"50c2a44a9e16c33f","x":4856,"y":1121,"width":195,"height":50},
		{"type":"text","text":"### Decision tree","id":"5a387ec33f161c31","x":4860,"y":1198,"width":195,"height":50},
		{"type":"text","text":"### KNN","id":"6c4f462a77c65d7c","x":4860,"y":1401,"width":195,"height":50},
		{"type":"text","text":"### SVM","id":"3daf0fac102c5527","x":4860,"y":1308,"width":148,"height":50},
		{"type":"text","text":"### Naiive Bayes","id":"15a55b18c74e3ac7","x":4860,"y":1485,"width":128,"height":85}
	],
	"edges":[
		{"id":"44ccb1ece13d1b33","fromNode":"0176eef9c6394291","fromSide":"bottom","toNode":"35afa7953c9f73f1","toSide":"top","label":"one variable"},
		{"id":"5fed454c48a0158e","fromNode":"35afa7953c9f73f1","fromSide":"right","toNode":"54f6685e9a74b7e7","toSide":"left","label":"driven by"},
		{"id":"1eaf908bf4d230dc","fromNode":"54f6685e9a74b7e7","fromSide":"bottom","toNode":"c0bdf6dcf747938d","toSide":"top","label":"works with"},
		{"id":"9116bb401b7c500d","fromNode":"1c4143ed866ab973","fromSide":"bottom","toNode":"973ee40c93fc1cd9","toSide":"top","label":"problem type 1"},
		{"id":"7eb7a4794cfbf428","fromNode":"c0bdf6dcf747938d","fromSide":"right","toNode":"993f8ab72d7b3cfb","toSide":"left"},
		{"id":"42d4614159891050","fromNode":"0176eef9c6394291","fromSide":"bottom","toNode":"0cbc1ee4808852ff","toSide":"top","label":"multiple\nvariables"},
		{"id":"9dffac325866f186","fromNode":"973ee40c93fc1cd9","fromSide":"bottom","toNode":"0176eef9c6394291","toSide":"top"},
		{"id":"3ed03c03405f0359","fromNode":"973ee40c93fc1cd9","fromSide":"bottom","toNode":"476a43ca8fb26d53","toSide":"left"},
		{"id":"a200bc3c41a65c48","fromNode":"05af23aaea7a20f7","fromSide":"right","toNode":"0cbc1ee4808852ff","toSide":"left","label":"used in"},
		{"id":"9120a95037281268","fromNode":"0cbc1ee4808852ff","fromSide":"bottom","toNode":"8ba726aeedb83a04","toSide":"top"},
		{"id":"16d7ed0d4ec41f7a","fromNode":"c0bdf6dcf747938d","fromSide":"left","toNode":"4c0c49f93c4ee8d6","toSide":"right"},
		{"id":"141343b304b12ae7","fromNode":"4c0c49f93c4ee8d6","fromSide":"top","toNode":"35afa7953c9f73f1","toSide":"bottom"},
		{"id":"56aa0a9bbebb6b90","fromNode":"72ebb3e3d6b2058b","fromSide":"left","toNode":"a9a6a839bcf4162b","toSide":"right","label":"unscaled"},
		{"id":"e736129c5c3acf8e","fromNode":"45f22145efd1381c","fromSide":"left","toNode":"b46217417c022682","toSide":"right","label":"scaled"},
		{"id":"a4f7f93a665858dc","fromNode":"72ebb3e3d6b2058b","fromSide":"bottom","toNode":"45f22145efd1381c","toSide":"top"},
		{"id":"d88391f920b4489a","fromNode":"45f22145efd1381c","fromSide":"left","toNode":"e5b14976b0d7ed55","toSide":"right","label":"implementation"},
		{"id":"62b3e2f353a425e1","fromNode":"e5b14976b0d7ed55","fromSide":"left","toNode":"9209659129726005","toSide":"top","label":"output"},
		{"id":"aeb6591f5ada2b6f","fromNode":"27c97ef539264f37","fromSide":"bottom","toNode":"72ebb3e3d6b2058b","toSide":"top"},
		{"id":"b5b6ca8f35d8d926","fromNode":"27c97ef539264f37","fromSide":"left","toNode":"6c20ba169c6e4188","toSide":"right"},
		{"id":"a09ecdd4af2d3f89","fromNode":"c0bdf6dcf747938d","fromSide":"bottom","toNode":"9e980a31b13e06b3","toSide":"top"},
		{"id":"062aec8dcd7ce26d","fromNode":"1c4143ed866ab973","fromSide":"right","toNode":"36d6caffca6a8dbb","toSide":"left","label":"problem type 2"},
		{"id":"cc1b5c499769b398","fromNode":"36d6caffca6a8dbb","fromSide":"right","toNode":"496d02c68a5c4da9","toSide":"left"},
		{"id":"2b2e3ec91c0de4ac","fromNode":"496d02c68a5c4da9","fromSide":"right","toNode":"59a338dfada827d5","toSide":"left"},
		{"id":"db0906aee1e87df5","fromNode":"59a338dfada827d5","fromSide":"right","toNode":"e13c667c9d042035","toSide":"left","label":"sigmoid function and \ncorresponding \ndecision boundary"},
		{"id":"d5aa60c887046c76","fromNode":"59a338dfada827d5","fromSide":"bottom","toNode":"c3f11a7d714f6d35","toSide":"top"},
		{"id":"1919faffa0d2aa2a","fromNode":"c3f11a7d714f6d35","fromSide":"right","toNode":"15d224b9fb475c34","toSide":"left"},
		{"id":"9e05fa3d750d62b2","fromNode":"27c97ef539264f37","fromSide":"left","toNode":"bd9136c8538f1282","toSide":"right"},
		{"id":"3084920a3ec8b7e6","fromNode":"bd9136c8538f1282","fromSide":"left","toNode":"1c8696753c59742d","toSide":"right"},
		{"id":"be67e8873eb70bc8","fromNode":"1c8696753c59742d","fromSide":"bottom","toNode":"993226b02f894ec9","toSide":"top"},
		{"id":"50964514cb40f619","fromNode":"993226b02f894ec9","fromSide":"left","toNode":"0c9f4e0779b8cbf9","toSide":"right"},
		{"id":"52c9b72a531d91ef","fromNode":"f48d63a2427d788c","fromSide":"top","toNode":"dabcf3923fc55e49","toSide":"bottom"},
		{"id":"ed72f0fae64bab8f","fromNode":"868b127a1d026bcb","fromSide":"right","toNode":"3bdb66d11398bb34","toSide":"left"},
		{"id":"6e8737f4976572f7","fromNode":"dabcf3923fc55e49","fromSide":"left","toNode":"8e01171f9dbc6704","toSide":"right"},
		{"id":"bd523d891bb3d349","fromNode":"270a3b62cbc269bd","fromSide":"left","toNode":"d1c08531261a06b7","toSide":"right"},
		{"id":"17a521b50406f246","fromNode":"6d223a6ecccfde5a","fromSide":"left","toNode":"d1c08531261a06b7","toSide":"right"},
		{"id":"b80e543c894f5f67","fromNode":"b8d255723be2141b","fromSide":"left","toNode":"d1c08531261a06b7","toSide":"right"},
		{"id":"df6f52b183f7e471","fromNode":"868b127a1d026bcb","fromSide":"top","toNode":"12913731189cd991","toSide":"bottom"},
		{"id":"38f9add1fff650aa","fromNode":"12913731189cd991","fromSide":"right","toNode":"ccc7ecbc63c10dd3","toSide":"left"},
		{"id":"6a1e914ac793c95b","fromNode":"ccc7ecbc63c10dd3","fromSide":"bottom","toNode":"3bdb66d11398bb34","toSide":"top"},
		{"id":"44b86a74e36192d3","fromNode":"31b104c1ba6c677a","fromSide":"right","toNode":"aa473df9ccf34c23","toSide":"left"},
		{"id":"bfa176647f61c759","fromNode":"31b104c1ba6c677a","fromSide":"bottom","toNode":"3552bb46c3fd2fef","toSide":"top"},
		{"id":"2c0d7d0be7694ad8","fromNode":"31b104c1ba6c677a","fromSide":"bottom","toNode":"cc98780ad53930d4","toSide":"top"},
		{"id":"6d5f54ee80cba5f4","fromNode":"31b104c1ba6c677a","fromSide":"bottom","toNode":"56d15d44c657d39b","toSide":"top"},
		{"id":"ddc24d35beb88be3","fromNode":"dabcf3923fc55e49","fromSide":"top","toNode":"e733ceb8d63fa346","toSide":"bottom"},
		{"id":"4f894b9056b82a77","fromNode":"e733ceb8d63fa346","fromSide":"right","toNode":"31b104c1ba6c677a","toSide":"left"},
		{"id":"180ec45893eb8b89","fromNode":"e733ceb8d63fa346","fromSide":"top","toNode":"a2e1d233f8e29c98","toSide":"bottom"},
		{"id":"e111ec687558b7a7","fromNode":"dabcf3923fc55e49","fromSide":"right","toNode":"baf6c1e4052843f7","toSide":"left","label":"f(z)"},
		{"id":"c36c4e18658ae7ed","fromNode":"baf6c1e4052843f7","fromSide":"right","toNode":"746b2fdd8175fa3a","toSide":"left"},
		{"id":"11367bf4a1547fdf","fromNode":"746b2fdd8175fa3a","fromSide":"right","toNode":"b02b092eb94b90de","toSide":"left"},
		{"id":"92295aea4a533c2b","fromNode":"07d5d14ae25284e8","fromSide":"bottom","toNode":"22e1a2f895acf05e","toSide":"top"},
		{"id":"73744e52d8221896","fromNode":"07d5d14ae25284e8","fromSide":"left","toNode":"27c97ef539264f37","toSide":"right"},
		{"id":"e0ca28edfbb87f36","fromNode":"07d5d14ae25284e8","fromSide":"top","toNode":"f48d63a2427d788c","toSide":"bottom"},
		{"id":"4874fa250d170bf1","fromNode":"07d5d14ae25284e8","fromSide":"left","toNode":"f51f992498b8187f","toSide":"right"},
		{"id":"ece1281283a3f6ce","fromNode":"22e1a2f895acf05e","fromSide":"bottom","toNode":"1c4143ed866ab973","toSide":"top"},
		{"id":"c0deb43e28b9e2f2","fromNode":"44bdf185f87d831b","fromSide":"right","toNode":"82e4fd2b7978ef6d","toSide":"left"},
		{"id":"6ee1da3224f3fda5","fromNode":"44bdf185f87d831b","fromSide":"bottom","toNode":"5797afa29b160333","toSide":"top"},
		{"id":"67b736920d1e65ee","fromNode":"44bdf185f87d831b","fromSide":"top","toNode":"7304b3a20e29c219","toSide":"bottom"},
		{"id":"a2793d305b040b91","fromNode":"44bdf185f87d831b","fromSide":"left","toNode":"455cad7a33c93b0c","toSide":"right"},
		{"id":"22317ea90630d4a4","fromNode":"44bdf185f87d831b","fromSide":"left","toNode":"03e1a3385909d0d3","toSide":"right"},
		{"id":"79792c404a745c3e","fromNode":"ad32b45e5becbf8f","fromSide":"right","toNode":"da2bb98cf2727930","toSide":"left"},
		{"id":"b941085490949e6d","fromNode":"ad32b45e5becbf8f","fromSide":"right","toNode":"3b69bad5825859ef","toSide":"left"},
		{"id":"34bd59971aba8282","fromNode":"82e4fd2b7978ef6d","fromSide":"right","toNode":"ad32b45e5becbf8f","toSide":"left"},
		{"id":"edc130815c4a2c20","fromNode":"c8889ed551acd0fa","fromSide":"right","toNode":"66b3548b53839827","toSide":"left"},
		{"id":"6388a13818c66ad7","fromNode":"c8889ed551acd0fa","fromSide":"bottom","toNode":"6c4f462a77c65d7c","toSide":"left"},
		{"id":"e8fb40711e7223f5","fromNode":"82e4fd2b7978ef6d","fromSide":"right","toNode":"c8889ed551acd0fa","toSide":"left"},
		{"id":"194c5be70bf1a0eb","fromNode":"82e4fd2b7978ef6d","fromSide":"bottom","toNode":"4e08ff18ae6e0430","toSide":"left"},
		{"id":"3cf39cc4c792964a","fromNode":"da2bb98cf2727930","fromSide":"right","toNode":"252e5d265aba4062","toSide":"left"},
		{"id":"9fcf003477c2fa43","fromNode":"3b69bad5825859ef","fromSide":"right","toNode":"252e5d265aba4062","toSide":"left"},
		{"id":"3c3434ac1892c06b","fromNode":"c8889ed551acd0fa","fromSide":"right","toNode":"50c2a44a9e16c33f","toSide":"left"},
		{"id":"ffbdd6d4fec75935","fromNode":"c8889ed551acd0fa","fromSide":"bottom","toNode":"3daf0fac102c5527","toSide":"left"},
		{"id":"6e123339351fc710","fromNode":"c8889ed551acd0fa","fromSide":"bottom","toNode":"15a55b18c74e3ac7","toSide":"left"},
		{"id":"4f79dc4a67286526","fromNode":"ad32b45e5becbf8f","fromSide":"right","toNode":"5a387ec33f161c31","toSide":"left"},
		{"id":"832655edf8dde945","fromNode":"66b3548b53839827","fromSide":"right","toNode":"252e5d265aba4062","toSide":"left"},
		{"id":"72d706c151571176","fromNode":"ad32b45e5becbf8f","fromSide":"right","toNode":"50c2a44a9e16c33f","toSide":"left"},
		{"id":"46a3d24f9d04363f","fromNode":"c8889ed551acd0fa","fromSide":"right","toNode":"5a387ec33f161c31","toSide":"left"}
	]
}